{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55fd671e",
   "metadata": {},
   "source": [
    "Preprocessing function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57fbfdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def preprocess_ocr_text(text) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # Replace known OCR noise characters\n",
    "    text = text.replace(\"â€œ\", \"\").replace(\"â€\", \"\").replace(\"â€˜\", \"\").replace(\"â€™\", \"'\")\n",
    "\n",
    "    # Remove unwanted symbols\n",
    "    text = re.sub(r\"(â|Â|¢|§|«|©|®|€|“|”|‘|’|™|…|_|=||•|—|–|@|%|<|>|\\\\|\\||~|`)\", \"\", text)\n",
    "\n",
    "    # Fix common formatting issues\n",
    "    text = re.sub(r\"(\\d)(tab/s|tablet[s]?|cap[s]?|capsule[s]?)\", r\"\\1 tablet\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"(\\d)(times)\", r\"\\1 times\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"(\\d)\\s*x\\s*(a|per)?\\s*day\", r\"\\1 times a day\", text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Normalize known expressions\n",
    "    replacements = {\n",
    "        \"twice a day\": \"2 times a day\",\n",
    "        \"three times daily\": \"3 times a day\",\n",
    "        \"when necessary\": \"when needed\",\n",
    "        \"when required\": \"when needed\",\n",
    "    }\n",
    "\n",
    "    for wrong, correct in replacements.items():\n",
    "        text = re.sub(rf\"\\b{wrong}\\b\", correct, text, flags=re.IGNORECASE)\n",
    "\n",
    "    #remove irrelevant data\n",
    "    lines = text.splitlines()\n",
    "    cleaned = []\n",
    "\n",
    "    for line in lines:\n",
    "        # Skip if line contains clinic/address info\n",
    "        if any(x in line for x in [\"clinic\", \"centre\", \"hospital\", \"#\", \"blk\", \"building\", \"road\", \"s \"]):\n",
    "            continue\n",
    "        # Skip prices and quantities\n",
    "        if re.search(r\"\\bqty\\b|\\bprice\\b|\\$\\d+|\\d+\\.\\d{2}\", line):\n",
    "            continue\n",
    "        cleaned.append(line)\n",
    "    \n",
    "\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "df = pd.read_csv(\"ocr2_output_cleaned.csv\")\n",
    "df[\"extracted_text\"] = df[\"extracted_text\"].apply(preprocess_ocr_text)\n",
    "df.to_csv(\"cleaned_ocr_labels2.csv\", index=False)\n",
    "\n",
    "#do not change to lowercase as it will affect the keyword matching later for medication!\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da11cc9",
   "metadata": {},
   "source": [
    "labeling of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1ae329b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Auto-labeling complete! Saved to auto_labeled_ner_data.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Define your CSV file and column\n",
    "csv_file = \"cleaned_ocr_labels2.csv\"\n",
    "column_name = \"extracted_text\"\n",
    "\n",
    "# Define keyword groups for automatic tagging\n",
    "keywords1 = {\n",
    "    \"DOSAGE\": [\"tablet\", \"tablets\", \"teblet\", \"tab\", \"tabs\", \"tab/s\",\"cap/s\",\"cap\", \"capsule\", \"capsules\"],\n",
    "    \"FREQUENCY\": [\"times\", \"time\", \"hour\", \"hours\",\"hourly\",\"morning\", \"evening\", \"afternoon\", \"bedtime\", \"night\"]\n",
    "}\n",
    "\n",
    "keywords2 = {\n",
    "    \"FREQUENCY\": [\"once\", \"twice\"],\n",
    "    \"INSTRUCTION\": [\"when\",\"needed\", \"after\", \"use\",\"before\", \"after\", \"with\", \"without\", \"meal\", \"meals\", \"food\", \"swallow\",\"chew\"],\n",
    "    \"NOTE\" : [\"fever\",\"pain\", \"cough\", \"cold\", \"flu\", \"runny\", \"allergy\", \"infection\", \"inflammation\", \"swelling\", \"sore throat\", \"headache\", \"nausea\",\"gastric\", \"drowsiness\", \"vomiting\", \"diarrhea\", \"constipation\", \"rash\", \"itching\", \"fatigue\", \"dizziness\"]\n",
    "}\n",
    "\n",
    "\n",
    "# split sentence into tokens using regex\n",
    "def tokenize(text):\n",
    "    return re.findall(r\"\\b\\w+(?:/\\w+)?\\b\", text.lower())\n",
    "\n",
    "# Auto-label function (basic BIO tagging)\n",
    "def auto_label(tokens):\n",
    "    labels = [\"O\"] * len(tokens)\n",
    "\n",
    "    quantity_words = {\n",
    "        \"1\", \"2\", \"3\", \"4\", \"5\", \"10\", \"one\", \"two\", \"three\", \"four\", \"five\", \"half\", \"quarter\"\n",
    "    }\n",
    "\n",
    "    # Collect known keywords\n",
    "    known_keywords = set()\n",
    "    for group in [*keywords1.values(), *keywords2.values()]:\n",
    "        known_keywords.update(word.lower() for word in group)\n",
    "\n",
    "    # ========== 1. MEDICATION_NAME tagging (first unknown word) ==========\n",
    "    for i, token in enumerate(tokens):\n",
    "        token_lower = token.lower()\n",
    "        if (\n",
    "            labels[i] == \"O\"\n",
    "            and token_lower not in known_keywords\n",
    "            and token_lower.isalpha()\n",
    "            and len(token) > 3\n",
    "        ):\n",
    "            labels[i] = \"B-MEDICATION_NAME\"\n",
    "            break  # only tag the first one\n",
    "\n",
    "    # ========== 2. DOSAGE tagging ==========\n",
    "    dosage_candidates = [\n",
    "        i for i in range(1, len(tokens))\n",
    "        if tokens[i].lower() in keywords1[\"DOSAGE\"] and tokens[i - 1].lower() in quantity_words\n",
    "    ]\n",
    "\n",
    "    if len(dosage_candidates) == 1:\n",
    "        i = dosage_candidates[0]\n",
    "        labels[i - 1] = \"B-DOSAGE\"\n",
    "        labels[i] = \"I-DOSAGE\"\n",
    "    elif len(dosage_candidates) >= 2:\n",
    "        i = dosage_candidates[1]  # tag only the second\n",
    "        labels[i - 1] = \"B-DOSAGE\"\n",
    "        labels[i] = \"I-DOSAGE\"\n",
    "\n",
    "    # ========== 3. FREQUENCY tagging ==========\n",
    "    for i in range(1, len(tokens)):\n",
    "        if (\n",
    "            tokens[i].lower() in keywords1[\"FREQUENCY\"] and \n",
    "            tokens[i - 1].lower()\n",
    "        ):\n",
    "            labels[i - 1] = \"B-FREQUENCY\"\n",
    "            labels[i] = \"I-FREQUENCY\"\n",
    "\n",
    "    # ========== 4. keywords2 tagging ==========\n",
    "    for i, token in enumerate(tokens):\n",
    "        token_lower = token.lower()\n",
    "        for label, keyword_list in keywords2.items():\n",
    "            if token_lower in keyword_list and labels[i] == \"O\":\n",
    "                labels[i] = f\"B-{label.upper()}\"\n",
    "                if (\n",
    "                    i + 1 < len(tokens) and \n",
    "                    tokens[i + 1].lower() in keyword_list\n",
    "                ):\n",
    "                    labels[i + 1] = f\"I-{label.upper()}\"\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Clean nulls and whitespace\n",
    "df = df[df[column_name].notnull()]\n",
    "df[column_name] = df[column_name].astype(str).str.strip()\n",
    "\n",
    "# Build token-label pairs\n",
    "data = []\n",
    "for text in df[column_name]:\n",
    "    tokens = tokenize(text)\n",
    "    labels = auto_label(tokens)\n",
    "    data.append({\"tokens\": tokens, \"labels\": labels})\n",
    "\n",
    "# Save to JSON\n",
    "with open(\"auto_labeled_ner_data2.json\", \"w\") as f:\n",
    "    json.dump(data, f, indent=2)\n",
    "\n",
    "print(\"✅ Auto-labeling complete! Saved to auto_labeled_ner_data.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "96ad5046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as auto_labeled_ner_data2_flat.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Load the JSON file\n",
    "with open(\"auto_labeled_ner_data2.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Flatten into token-label pairs\n",
    "rows = []\n",
    "for entry in data:\n",
    "    tokens = entry[\"tokens\"]\n",
    "    labels = entry[\"labels\"]\n",
    "    rows.extend(zip(tokens, labels))\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(rows, columns=[\"Token\", \"Label\"])\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"auto_labeled_ner_data2_flat.csv\", index=False)\n",
    "\n",
    "print(\"Saved as auto_labeled_ner_data2_flat.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "50038be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Converted 1 samples to JSON format.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the flat CSV\n",
    "df = pd.read_csv(\"auto_labeled_ner_data3_flat_cleaned.csv\")\n",
    "\n",
    "# Fill missing values to avoid NaNs\n",
    "df.fillna(\"\", inplace=True)\n",
    "\n",
    "# Split into samples using blank lines as boundaries\n",
    "data = []\n",
    "current_tokens = []\n",
    "current_labels = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    token = row[\"Token\"].strip()\n",
    "    label = row[\"Label\"].strip()\n",
    "\n",
    "    if token == \"\":  # new sample\n",
    "        if current_tokens:\n",
    "            data.append({\"tokens\": current_tokens, \"labels\": current_labels})\n",
    "            current_tokens = []\n",
    "            current_labels = []\n",
    "    else:\n",
    "        current_tokens.append(token)\n",
    "        current_labels.append(label)\n",
    "\n",
    "# Add the last one if not already added\n",
    "if current_tokens:\n",
    "    data.append({\"tokens\": current_tokens, \"labels\": current_labels})\n",
    "\n",
    "# Save to JSON\n",
    "with open(\"auto_labeled_ner_data3.json\", \"w\") as f:\n",
    "    json.dump(data, f, indent=2)\n",
    "\n",
    "print(f\"✅ Converted {len(data)} samples to JSON format.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e72e03de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Converted 84 medication blocks. Saved to: C:\\Users\\prisc\\OneDrive\\Desktop\\Github\\MediMind\\ml\\clean_clean.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Path to your raw data\n",
    "input_path = r\"C:\\Users\\prisc\\OneDrive\\Desktop\\Github\\MediMind\\ml\\auto_labeled_ner_data3_flat.csv\"\n",
    "output_path = r\"C:\\Users\\prisc\\OneDrive\\Desktop\\Github\\MediMind\\ml\\clean_clean.json\"\n",
    "\n",
    "# Parse and group tokens by B-MEDICATION_NAME start\n",
    "data_blocks = []\n",
    "current_tokens = []\n",
    "current_labels = []\n",
    "\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line or line.lower().startswith(\"token\"):  # Skip empty lines/header\n",
    "            continue\n",
    "        try:\n",
    "            token, label = line.split(\",\", 1)\n",
    "            token = token.strip()\n",
    "            label = label.strip().upper()\n",
    "\n",
    "            if label == \"B-MEDICATION_NAME\":\n",
    "                # Start new block if one already exists\n",
    "                if current_tokens:\n",
    "                    data_blocks.append({\n",
    "                        \"tokens\": current_tokens,\n",
    "                        \"labels\": current_labels\n",
    "                    })\n",
    "                    current_tokens = []\n",
    "                    current_labels = []\n",
    "\n",
    "            current_tokens.append(token)\n",
    "            current_labels.append(label)\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "# Append final group\n",
    "if current_tokens:\n",
    "    data_blocks.append({\n",
    "        \"tokens\": current_tokens,\n",
    "        \"labels\": current_labels\n",
    "    })\n",
    "\n",
    "# Save as JSON\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as out:\n",
    "    json.dump(data_blocks, out, indent=2)\n",
    "\n",
    "print(f\"✅ Converted {len(data_blocks)} medication blocks. Saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "faf0f929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (4.54.1)\n",
      "Requirement already satisfied: datasets in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (4.0.0)\n",
      "Requirement already satisfied: seqeval in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (1.2.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (1.6.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (0.34.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\python313\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\python313\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.14)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: colorama in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\python313\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\python313\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\python313\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets seqeval scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb0b9f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: accelerate>=0.26.0 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (1.9.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from accelerate>=0.26.0) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\python313\\lib\\site-packages (from accelerate>=0.26.0) (25.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from accelerate>=0.26.0) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from accelerate>=0.26.0) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from accelerate>=0.26.0) (2.7.1)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from accelerate>=0.26.0) (0.34.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from accelerate>=0.26.0) (0.5.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0) (2025.3.0)\n",
      "Requirement already satisfied: requests in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from torch>=2.0.0->accelerate>=0.26.0) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from torch>=2.0.0->accelerate>=0.26.0) (80.8.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate>=0.26.0) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from tqdm>=4.42.1->huggingface_hub>=0.21.0->accelerate>=0.26.0) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from jinja2->torch>=2.0.0->accelerate>=0.26.0) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate>=0.26.0) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate>=0.26.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate>=0.26.0) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate>=0.26.0) (2025.4.26)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install \"accelerate>=0.26.0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "012a4b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (4.54.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (0.34.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\python313\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from requests->transformers) (2025.4.26)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f0649c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torchvision in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (0.22.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from torchvision) (2.2.6)\n",
      "Requirement already satisfied: torch==2.7.1 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from torchvision) (2.7.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from torchvision) (11.2.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from torch==2.7.1->torchvision) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from torch==2.7.1->torchvision) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from torch==2.7.1->torchvision) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from torch==2.7.1->torchvision) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from torch==2.7.1->torchvision) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from torch==2.7.1->torchvision) (2025.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from torch==2.7.1->torchvision) (80.8.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from sympy>=1.13.3->torch==2.7.1->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from jinja2->torch==2.7.1->torchvision) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (2.7.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from torch) (80.8.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\prisc\\appdata\\roaming\\python\\python313\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision\n",
    "!pip install torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26a634c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.7.1\n",
      "Uninstalling torch-2.7.1:\n",
      "  Successfully uninstalled torch-2.7.1\n",
      "Found existing installation: torchvision 0.22.1\n",
      "Uninstalling torchvision-0.22.1:\n",
      "  Successfully uninstalled torchvision-0.22.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping torchaudio as it is not installed.\n"
     ]
    }
   ],
   "source": [
    "pip uninstall torch torchvision torchaudio -y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c616e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.1+cpu\n",
      "0.22.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d33b5306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Labels cleaned and saved to auto_labeled_ner_data3_flat_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(\"auto_labeled_ner_data3_flat.csv\")\n",
    "\n",
    "# Clean 'Label' column\n",
    "df[\"Label\"] = df[\"Label\"].astype(str).str.upper().replace(\"O\", \"O\")\n",
    "\n",
    "# Save cleaned CSV\n",
    "df.to_csv(\"auto_labeled_ner_data3_flat_cleaned.csv\", index=False)\n",
    "\n",
    "print(\"✅ Labels cleaned and saved to auto_labeled_ner_data3_flat_cleaned.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c874d5",
   "metadata": {},
   "source": [
    "training BERT model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9a6b90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label list: ['B-DOSAGE', 'B-FREQUENCY', 'B-INSTRUCTION', 'B-MEDICATION_NAME', 'B-NOTE', 'I-DOSAGE', 'I-FREQUENCY', 'I-INSTRUCTION', 'I-MEDICATION_NAME', 'I-NOTE', 'O']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59e342b678024460858bd9f667ea3aad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/84 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\prisc\\AppData\\Local\\Temp\\ipykernel_12404\\1794456460.py:57: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "C:\\Users\\prisc\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='33' max='33' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [33/33 09:26, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.392200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.665900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.422500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "SafetensorError",
     "evalue": "Error while serializing: IoError(Os { code: 1224, kind: Uncategorized, message: \"The requested operation cannot be performed on a file with a user-mapped section open.\" })",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mSafetensorError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 68\u001b[39m\n\u001b[32m     65\u001b[39m trainer.train()\n\u001b[32m     67\u001b[39m \u001b[38;5;66;03m#save the model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./ner_medication_model\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m tokenizer.save_pretrained(\u001b[33m\"\u001b[39m\u001b[33m./ner_medication_model\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     72\u001b[39m \u001b[38;5;66;03m#inference testing\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\trainer.py:3987\u001b[39m, in \u001b[36mTrainer.save_model\u001b[39m\u001b[34m(self, output_dir, _internal_call)\u001b[39m\n\u001b[32m   3984\u001b[39m         \u001b[38;5;28mself\u001b[39m.model_wrapped.save_checkpoint(output_dir)\n\u001b[32m   3986\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.should_save:\n\u001b[32m-> \u001b[39m\u001b[32m3987\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3989\u001b[39m \u001b[38;5;66;03m# Push to the Hub when `save_model` is called by the user.\u001b[39;00m\n\u001b[32m   3990\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.push_to_hub \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _internal_call:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\trainer.py:4091\u001b[39m, in \u001b[36mTrainer._save\u001b[39m\u001b[34m(self, output_dir, state_dict)\u001b[39m\n\u001b[32m   4089\u001b[39m             torch.save(state_dict, os.path.join(output_dir, WEIGHTS_NAME))\n\u001b[32m   4090\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4091\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4092\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_serialization\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_safetensors\u001b[49m\n\u001b[32m   4093\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4095\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.processing_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4096\u001b[39m     \u001b[38;5;28mself\u001b[39m.processing_class.save_pretrained(output_dir)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\modeling_utils.py:4107\u001b[39m, in \u001b[36mPreTrainedModel.save_pretrained\u001b[39m\u001b[34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001b[39m\n\u001b[32m   4102\u001b[39m     gc.collect()\n\u001b[32m   4104\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m safe_serialization:\n\u001b[32m   4105\u001b[39m     \u001b[38;5;66;03m# At some point we will need to deal better with save_function (used for TPU and other distributed\u001b[39;00m\n\u001b[32m   4106\u001b[39m     \u001b[38;5;66;03m# joyfulness), but for now this enough.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4107\u001b[39m     \u001b[43msafe_save_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mformat\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4108\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   4109\u001b[39m     save_function(shard, os.path.join(save_directory, shard_file))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\safetensors\\torch.py:286\u001b[39m, in \u001b[36msave_file\u001b[39m\u001b[34m(tensors, filename, metadata)\u001b[39m\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msave_file\u001b[39m(\n\u001b[32m    256\u001b[39m     tensors: Dict[\u001b[38;5;28mstr\u001b[39m, torch.Tensor],\n\u001b[32m    257\u001b[39m     filename: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    258\u001b[39m     metadata: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    259\u001b[39m ):\n\u001b[32m    260\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    261\u001b[39m \u001b[33;03m    Saves a dictionary of tensors into raw bytes in safetensors format.\u001b[39;00m\n\u001b[32m    262\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    284\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    285\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m     \u001b[43mserialize_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_flatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mSafetensorError\u001b[39m: Error while serializing: IoError(Os { code: 1224, kind: Uncategorized, message: \"The requested operation cannot be performed on a file with a user-mapped section open.\" })"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "from transformers import BertTokenizerFast\n",
    "from transformers import BertForTokenClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import pipeline\n",
    "\n",
    "with open(\"clean_clean.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "dataset = Dataset.from_list(data)\n",
    "dataset\n",
    "\n",
    "#create a label list\n",
    "label_list = sorted({label for d in data for label in d[\"labels\"]})\n",
    "label_to_id = {label: i for i, label in enumerate(label_list)}\n",
    "id_to_label = {i: label for label, i in label_to_id.items()}\n",
    "\n",
    "print(\"Label list:\", label_list)\n",
    "\n",
    "#tokenise and align labels\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "def tokenize_and_align_labels(example):\n",
    "    tokenized_inputs = tokenizer(example[\"tokens\"], is_split_into_words=True, padding='max_length',  truncation=True)\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "    \n",
    "    labels = []\n",
    "    previous_word_idx = None\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            labels.append(-100)\n",
    "        elif word_idx != previous_word_idx:\n",
    "            labels.append(label_to_id[example[\"labels\"][word_idx]])\n",
    "        else:\n",
    "            labels.append(label_to_id[example[\"labels\"][word_idx]])\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_and_align_labels)\n",
    "\n",
    "#load the model with tokenized labels\n",
    "model = BertForTokenClassification.from_pretrained(\"bert-base-cased\", num_labels=11)\n",
    "\n",
    "#train the model\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ner_model\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"epoch\"\n",
    "    \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "#save the model\n",
    "trainer.save_model(\"./ner_medication_model\")\n",
    "tokenizer.save_pretrained(\"./ner_medication_model\")\n",
    "\n",
    "\n",
    "#inference testing\n",
    "ner_pipe = pipeline(\"ner\", model=\"./ner_medication_model\", tokenizer=\"./ner_medication_model\", aggregation_strategy=\"simple\")\n",
    "\n",
    "text = \"Take 2 tablets 3 times a day after food\"\n",
    "results = ner_pipe(text)\n",
    "\n",
    "for entity in results:\n",
    "    print(entity[\"word\"], \"→\", entity[\"entity_group\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "324f9ab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "264df802ff9e4e939e47c3fda282ef45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/67 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "230187f55de3434a95edcd93b060cd4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/17 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 84\u001b[39m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     77\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m\"\u001b[39m: accuracy_score(true_labels, true_predictions),\n\u001b[32m     78\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mprecision\u001b[39m\u001b[33m\"\u001b[39m: precision_score(true_labels, true_predictions),\n\u001b[32m     79\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrecall\u001b[39m\u001b[33m\"\u001b[39m: recall_score(true_labels, true_predictions),\n\u001b[32m     80\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mf1\u001b[39m\u001b[33m\"\u001b[39m: f1_score(true_labels, true_predictions),\n\u001b[32m     81\u001b[39m     }\n\u001b[32m     83\u001b[39m \u001b[38;5;66;03m# ===== 8. Training Arguments =====\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m training_args = \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./ner_model\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mepoch\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mepoch\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./logs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mepoch\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     93\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;66;03m# ===== 9. Trainer Setup =====\u001b[39;00m\n\u001b[32m     96\u001b[39m trainer = Trainer(\n\u001b[32m     97\u001b[39m     model=model,\n\u001b[32m     98\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m    102\u001b[39m     compute_metrics=compute_metrics\n\u001b[32m    103\u001b[39m )\n",
      "\u001b[31mTypeError\u001b[39m: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from seqeval.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# ===== 1. Load Data =====\n",
    "with open(\"clean_clean.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# ===== 2. Split into Train and Eval =====\n",
    "train_data, eval_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_list(train_data),\n",
    "    \"eval\": Dataset.from_list(eval_data)\n",
    "})\n",
    "\n",
    "# ===== 3. Create Label Mapping =====\n",
    "label_list = sorted({label for d in data for label in d[\"labels\"]})\n",
    "label_to_id = {label: i for i, label in enumerate(label_list)}\n",
    "id_to_label = {i: label for label, i in label_to_id.items()}\n",
    "num_labels = len(label_list)\n",
    "\n",
    "# ===== 4. Tokenizer =====\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# ===== 5. Tokenization & Label Alignment =====\n",
    "def tokenize_and_align_labels(example):\n",
    "    tokenized_inputs = tokenizer(example[\"tokens\"], is_split_into_words=True, padding='max_length', truncation=True)\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "\n",
    "    labels = []\n",
    "    previous_word_idx = None\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            labels.append(-100)\n",
    "        else:\n",
    "            labels.append(label_to_id[example[\"labels\"][word_idx]])\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_and_align_labels)\n",
    "\n",
    "# ===== 6. Load Model =====\n",
    "model = BertForTokenClassification.from_pretrained(\n",
    "    \"bert-base-cased\",\n",
    "    num_labels=num_labels,\n",
    "    id2label=id_to_label,\n",
    "    label2id=label_to_id\n",
    ")\n",
    "\n",
    "# ===== 7. Compute Metrics =====\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = predictions.argmax(axis=-1)\n",
    "\n",
    "    true_labels = []\n",
    "    true_predictions = []\n",
    "\n",
    "    for pred, label in zip(predictions, labels):\n",
    "        current_preds = []\n",
    "        current_labels = []\n",
    "        for p_i, l_i in zip(pred, label):\n",
    "            if l_i != -100:\n",
    "                current_preds.append(id_to_label[p_i])\n",
    "                current_labels.append(id_to_label[l_i])\n",
    "        true_predictions.append(current_preds)\n",
    "        true_labels.append(current_labels)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(true_labels, true_predictions),\n",
    "        \"precision\": precision_score(true_labels, true_predictions),\n",
    "        \"recall\": recall_score(true_labels, true_predictions),\n",
    "        \"f1\": f1_score(true_labels, true_predictions),\n",
    "    }\n",
    "\n",
    "# ===== 8. Training Arguments =====\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ner_model\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "# ===== 9. Trainer Setup =====\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"eval\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# ===== 10. Train Model =====\n",
    "trainer.train()\n",
    "\n",
    "# ===== 11. Save Model =====\n",
    "model_path = \"./ner_medication_model\"\n",
    "trainer.save_model(model_path)\n",
    "tokenizer.save_pretrained(model_path)\n",
    "\n",
    "# ===== 12. Evaluate on Eval Set =====\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"\\n📊 Evaluation Metrics:\")\n",
    "for k, v in eval_results.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "# ===== 13. Inference Example =====\n",
    "ner_pipe = pipeline(\"ner\", model=model_path, tokenizer=model_path, aggregation_strategy=\"simple\")\n",
    "text = \"Take 2 tablets 3 times a day after food\"\n",
    "results = ner_pipe(text)\n",
    "\n",
    "print(\"\\n📌 Inference Output:\")\n",
    "for entity in results:\n",
    "    print(f\"{entity['word']} → {entity['entity_group']}\")\n",
    "\n",
    "# ===== 14. Extract Trainer Logs =====\n",
    "logs = trainer.state.log_history\n",
    "df_logs = pd.DataFrame(logs)\n",
    "\n",
    "# ===== 15. Plot Loss and F1 =====\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot training loss\n",
    "if \"loss\" in df_logs.columns:\n",
    "    plt.plot(df_logs[\"step\"], df_logs[\"loss\"], label=\"Training Loss\", marker='o')\n",
    "\n",
    "# Plot evaluation loss\n",
    "if \"eval_loss\" in df_logs.columns:\n",
    "    plt.plot(df_logs[\"step\"], df_logs[\"eval_loss\"], label=\"Eval Loss\", marker='x')\n",
    "\n",
    "# Plot F1 score\n",
    "if \"eval_f1\" in df_logs.columns:\n",
    "    plt.plot(df_logs[\"step\"], df_logs[\"eval_f1\"], label=\"Eval F1 Score\", marker='s')\n",
    "\n",
    "# ===== 16. Final Plot Styling =====\n",
    "plt.title(\"Training/Eval Loss and F1 Score Over Time\")\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "433a95ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'which' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!which python\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58417361",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from collections import defaultdict\n",
    "\n",
    "# ===== 1. Load BERT NER pipeline =====\n",
    "ner_pipe = pipeline(\n",
    "    \"ner\",\n",
    "    model=\"./ner_medication_model\",\n",
    "    tokenizer=\"./ner_medication_model\",\n",
    "    aggregation_strategy=\"simple\"  # ensures \"2 tablets\" is treated as one\n",
    ")\n",
    "\n",
    "# ===== 2. Define input text (replace with dynamic input later) =====\n",
    "text = \"Take 2 tablets 3 times a day after food loratadine may cause drowsiness\"\n",
    "\n",
    "# ===== 3. Run inference =====\n",
    "results = ner_pipe(text)\n",
    "\n",
    "# ===== 4. Group output by entity type =====\n",
    "grouped_output = defaultdict(list)\n",
    "\n",
    "for entity in results:\n",
    "    label = entity[\"entity_group\"]  # e.g. B-DOSAGE\n",
    "    word = entity[\"word\"]\n",
    "\n",
    "    # Normalize label (strip B- or I- prefixes)\n",
    "    base_label = label.split(\"-\")[-1] if \"-\" in label else label\n",
    "    grouped_output[base_label].append(word)\n",
    "\n",
    "# ===== 5. Format results for display =====\n",
    "print(\"📱 Output for Mobile App:\\n\")\n",
    "final_output = {}\n",
    "\n",
    "for field in [\"MEDICATION_NAME\", \"DOSAGE\", \"FREQUENCY\", \"INSTRUCTION\", \"NOTE\"]:\n",
    "    value = \" \".join(grouped_output.get(field, []))\n",
    "    if value:\n",
    "        final_output[field.lower()] = value\n",
    "        print(f\"{field.capitalize()}: {value}\")\n",
    "\n",
    "# ===== 6. (Optional) Use as JSON in your app =====\n",
    "# print(final_output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
