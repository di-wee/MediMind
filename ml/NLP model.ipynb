{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55fd671e",
   "metadata": {},
   "source": [
    "Preprocessing function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fbfdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def preprocess_ocr_text(text) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # Replace known OCR noise characters\n",
    "    text = text.replace(\"â€œ\", \"\").replace(\"â€\", \"\").replace(\"â€˜\", \"\").replace(\"â€™\", \"'\")\n",
    "\n",
    "    # Remove unwanted symbols\n",
    "    text = re.sub(r\"(â|Â|¢|§|«|©|®|€|“|”|‘|’|™|…|_|=||•|—|–|@|%|<|>|\\\\|\\||~|`)\", \"\", text)\n",
    "\n",
    "    # Fix common formatting issues\n",
    "    text = re.sub(r\"(\\d)(tab/s|tablet[s]?|cap[s]?|capsule[s]?)\", r\"\\1 tablet\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"(\\d)(times)\", r\"\\1 times\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"(\\d)\\s*x\\s*(a|per)?\\s*day\", r\"\\1 times a day\", text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Normalize known expressions\n",
    "    replacements = {\n",
    "        \"twice a day\": \"2 times a day\",\n",
    "        \"three times daily\": \"3 times a day\",\n",
    "        \"when necessary\": \"when needed\",\n",
    "        \"when required\": \"when needed\",\n",
    "    }\n",
    "\n",
    "    for wrong, correct in replacements.items():\n",
    "        text = re.sub(rf\"\\b{wrong}\\b\", correct, text, flags=re.IGNORECASE)\n",
    "\n",
    "    #remove irrelevant data\n",
    "    lines = text.lower().splitlines()\n",
    "    cleaned = []\n",
    "\n",
    "    for line in lines:\n",
    "        # Skip if line contains clinic/address info\n",
    "        if any(x in line for x in [\"clinic\", \"centre\", \"hospital\", \"#\", \"blk\", \"building\", \"road\", \"s \"]):\n",
    "            continue\n",
    "        # Skip prices and quantities\n",
    "        if re.search(r\"\\bqty\\b|\\bprice\\b|\\$\\d+|\\d+\\.\\d{2}\", line):\n",
    "            continue\n",
    "        cleaned.append(line)\n",
    "    \n",
    "\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    return text.strip().lower()\n",
    "\n",
    "df = pd.read_csv(\"ocr_labels.csv\")\n",
    "df[\"extracted_text\"] = df[\"extracted_text\"].apply(preprocess_ocr_text)\n",
    "df.to_csv(\"cleaned_ocr_labels.csv\", index=False)    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da11cc9",
   "metadata": {},
   "source": [
    "labeling of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae329b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Auto-labeling complete! Saved to auto_labeled_ner_data.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Define your CSV file and column\n",
    "csv_file = \"50cleaned_ocr_labels.csv\"\n",
    "column_name = \"extracted_text\"\n",
    "\n",
    "# Define keyword groups for automatic tagging\n",
    "keywords = {\n",
    "    \"DOSAGE\": [\"tablet\", \"tablets\", \"tab\", \"tabs\", \"capsule\", \"capsules\", \"ml\", \"mg\"],\n",
    "    \"FREQUENCY\": [\"once\", \"twice\", \"daily\", \"every\", \"hour\", \"hours\", \"day\", \"night\", \"morning\", \"evening\"],\n",
    "    \"TIMING\": [\"before\", \"after\", \"with\", \"without\", \"meal\", \"meals\", \"food\"],\n",
    "    \"MEDICATION_NAME\": [\"ibuprofen\",\"montelukast\",\"prednisolone\",\"telfast\",\"celebrax\",\"augmentin\", \"paracetamol\", \"enhancin\", \"amoxicillin\"]  # Add more\n",
    "}\n",
    "\n",
    "# split sentence into tokens using regex\n",
    "def tokenize(text):\n",
    "    return re.findall(r\"\\b\\w+(?:/\\w+)?\\b\", text.lower())\n",
    "\n",
    "# Auto-label function (basic BIO tagging)\n",
    "def auto_label(tokens):\n",
    "    labels = [\"O\"] * len(tokens)\n",
    "    for i, token in enumerate(tokens):\n",
    "        for label, keyword_list in keywords.items():\n",
    "            if token in keyword_list:\n",
    "                labels[i] = f\"B-{label}\"\n",
    "                # Check for multi-word terms\n",
    "                if i+1 < len(tokens) and tokens[i+1] in keyword_list:\n",
    "                    labels[i+1] = f\"I-{label}\"\n",
    "    return labels\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Clean nulls and whitespace\n",
    "df = df[df[column_name].notnull()]\n",
    "df[column_name] = df[column_name].astype(str).str.strip()\n",
    "\n",
    "# Build token-label pairs\n",
    "data = []\n",
    "for text in df[column_name]:\n",
    "    tokens = tokenize(text)\n",
    "    labels = auto_label(tokens)\n",
    "    data.append({\"tokens\": tokens, \"labels\": labels})\n",
    "\n",
    "# Save to JSON\n",
    "with open(\"auto_labeled_ner_data.json\", \"w\") as f:\n",
    "    json.dump(data, f, indent=2)\n",
    "\n",
    "print(\"✅ Auto-labeling complete! Saved to auto_labeled_ner_data.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "faf0f929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\prisc\\anaconda3\\lib\\site-packages (4.53.3)\n",
      "Requirement already satisfied: datasets in c:\\users\\prisc\\anaconda3\\lib\\site-packages (4.0.0)\n",
      "Requirement already satisfied: seqeval in c:\\users\\prisc\\anaconda3\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\prisc\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from transformers) (0.34.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: colorama in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.11.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets seqeval scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb0b9f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate>=0.26.0 in c:\\users\\prisc\\anaconda3\\lib\\site-packages (1.9.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from accelerate>=0.26.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from accelerate>=0.26.0) (24.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from accelerate>=0.26.0) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from accelerate>=0.26.0) (6.0.1)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from accelerate>=0.26.0) (2.7.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from accelerate>=0.26.0) (0.34.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from accelerate>=0.26.0) (0.5.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0) (2024.6.1)\n",
      "Requirement already satisfied: requests in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0) (4.11.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from torch>=2.0.0->accelerate>=0.26.0) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from torch>=2.0.0->accelerate>=0.26.0) (75.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate>=0.26.0) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub>=0.21.0->accelerate>=0.26.0) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate>=0.26.0) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate>=0.26.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate>=0.26.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate>=0.26.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate>=0.26.0) (2025.4.26)\n"
     ]
    }
   ],
   "source": [
    "!pip install \"accelerate>=0.26.0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "012a4b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\prisc\\anaconda3\\lib\\site-packages (4.53.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from transformers) (0.34.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\prisc\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.4.26)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c874d5",
   "metadata": {},
   "source": [
    "training BERT model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9a6b90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\prisc\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Label list: ['B-DOSAGE', 'B-FREQUENCY', 'B-MEDICATION_NAME', 'B-TIMING', 'O']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50c369d7bb2b4c7ba9a3d07242a419d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/51 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\prisc\\AppData\\Local\\Temp\\ipykernel_25684\\4132790422.py:57: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "c:\\Users\\prisc\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21' max='21' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [21/21 02:52, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.012000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.227700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.141200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Take 2 tablets 3 times a → LABEL_4\n",
      "day → LABEL_1\n",
      "after → LABEL_3\n",
      "food → LABEL_4\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "from transformers import BertTokenizerFast\n",
    "from transformers import BertForTokenClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import pipeline\n",
    "\n",
    "with open(\"auto_labeled_ner_data.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "dataset = Dataset.from_list(data)\n",
    "dataset\n",
    "\n",
    "#create a label list\n",
    "label_list = sorted({label for d in data for label in d[\"labels\"]})\n",
    "label_to_id = {label: i for i, label in enumerate(label_list)}\n",
    "id_to_label = {i: label for label, i in label_to_id.items()}\n",
    "\n",
    "print(\"Label list:\", label_list)\n",
    "\n",
    "#tokenise and align labels\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "def tokenize_and_align_labels(example):\n",
    "    tokenized_inputs = tokenizer(example[\"tokens\"], is_split_into_words=True, padding='max_length',  truncation=True)\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "    \n",
    "    labels = []\n",
    "    previous_word_idx = None\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            labels.append(-100)\n",
    "        elif word_idx != previous_word_idx:\n",
    "            labels.append(label_to_id[example[\"labels\"][word_idx]])\n",
    "        else:\n",
    "            labels.append(label_to_id[example[\"labels\"][word_idx]])\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_and_align_labels)\n",
    "\n",
    "#load the model with tokenized labels\n",
    "model = BertForTokenClassification.from_pretrained(\"bert-base-cased\", num_labels=9)\n",
    "\n",
    "#train the model\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ner_model\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"epoch\"\n",
    "    \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "#save the model\n",
    "trainer.save_model(\"./ner_medication_model\")\n",
    "tokenizer.save_pretrained(\"./ner_medication_model\")\n",
    "\n",
    "\n",
    "#inference testing\n",
    "ner_pipe = pipeline(\"ner\", model=\"./ner_medication_model\", tokenizer=\"./ner_medication_model\", aggregation_strategy=\"simple\")\n",
    "\n",
    "text = \"Take 2 tablets 3 times a day after food\"\n",
    "results = ner_pipe(text)\n",
    "\n",
    "for entity in results:\n",
    "    print(entity[\"word\"], \"→\", entity[\"entity_group\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
