{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15e7aa0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing Current Model with Improved Processing ===\n",
      "Original noisy text:\n",
      "KEEP FluVOXAMine MALEATE SOMG TAB\n",
      "STEP 3: TAKE HALF TABLET EVERY OTHER NIGHT\n",
      "TAKE WITH OR AFTER FOOD. AVOID ALCOHOL.\n",
      "TEL: 6930 2262\n",
      "\n",
      "==================================================\n",
      "Cleaned text: KEEP FluVOXAMine MALEATE SOMG TABLET STEP 3: TAKE HALF TABLET EVERY OTHER NIGHT TAKE WITH OR AFTER FOOD. AVOID ALCOHOL. TEL: 6930 2262\n",
      "\n",
      "Improved NER Results:\n",
      "\n",
      "Grouped Results:\n",
      "\n",
      "Extracted Information:\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Quick test of current model with improved preprocessing\n",
    "No retraining required!\n",
    "\"\"\"\n",
    "\n",
    "from transformers import pipeline, BertTokenizerFast, BertForTokenClassification\n",
    "import re\n",
    "\n",
    "def test_current_model_with_improvements():\n",
    "    \"\"\"Test current model with improved preprocessing\"\"\"\n",
    "    \n",
    "    print(\"=== Testing Current Model with Improved Processing ===\")\n",
    "    \n",
    "    # Load your current model directly to handle the size mismatch\n",
    "    model_path = r\"C:\\Users\\prisc\\OneDrive\\Desktop\\Github\\MediMind\\ml\\new_tesseract_ner\\ner_model5\"\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "    \n",
    "    # Create model with correct number of labels (9 based on the saved weights)\n",
    "    model = BertForTokenClassification.from_pretrained(\n",
    "        model_path, \n",
    "        num_labels=11,  # Use 9 labels to match the saved weights\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "    ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"none\")\n",
    "    \n",
    "    def preprocess_text(text):\n",
    "        \"\"\"Clean and preprocess OCR text to reduce noise\"\"\"\n",
    "        # Remove excessive whitespace and newlines\n",
    "        text = re.sub(r'\\n+', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        # Remove common OCR artifacts\n",
    "        text = re.sub(r'[^\\w\\s\\-\\.\\,\\:\\;\\(\\)\\/]', '', text)\n",
    "        \n",
    "        # Normalize common medication-related terms\n",
    "        text = re.sub(r'\\bTAB\\b', 'TABLET', text, flags=re.IGNORECASE)\n",
    "        text = re.sub(r'\\bMG\\b', 'MG', text)\n",
    "        text = re.sub(r'\\bML\\b', 'ML', text)\n",
    "        \n",
    "        return text.strip()\n",
    "    \n",
    "    def enhanced_ner_processing(text, confidence_threshold=0.6):\n",
    "        \"\"\"Enhanced NER processing with better noise handling\"\"\"\n",
    "        \n",
    "        # Preprocess text\n",
    "        cleaned_text = preprocess_text(text)\n",
    "        print(f\"Cleaned text: {cleaned_text}\")\n",
    "        \n",
    "        # Get NER results\n",
    "        ner_results = ner_pipeline(cleaned_text)\n",
    "        \n",
    "        # Filter by confidence and merge subwords\n",
    "        filtered_results = []\n",
    "        current_entity = None\n",
    "        \n",
    "        for result in ner_results:\n",
    "            if result['score'] >= confidence_threshold:\n",
    "                word = result['word']\n",
    "                entity = result['entity']\n",
    "                \n",
    "                # Handle subword tokens\n",
    "                if word.startswith('##'):\n",
    "                    if current_entity:\n",
    "                        current_entity['word'] += word[2:]\n",
    "                else:\n",
    "                    if current_entity:\n",
    "                        filtered_results.append(current_entity)\n",
    "                    current_entity = {\n",
    "                        'word': word,\n",
    "                        'entity': entity,\n",
    "                        'score': result['score']\n",
    "                    }\n",
    "        \n",
    "        if current_entity:\n",
    "            filtered_results.append(current_entity)\n",
    "        \n",
    "        return filtered_results\n",
    "    \n",
    "    # Your noisy input\n",
    "    noisy_text = \"\"\"KEEP FluVOXAMine MALEATE SOMG TAB\n",
    "STEP 3: TAKE HALF TABLET EVERY OTHER NIGHT\n",
    "TAKE WITH OR AFTER FOOD. AVOID ALCOHOL.\n",
    "TEL: 6930 2262\"\"\"\n",
    "    \n",
    "    print(\"Original noisy text:\")\n",
    "    print(noisy_text)\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    \n",
    "    # Test with improved processing\n",
    "    results = enhanced_ner_processing(noisy_text)\n",
    "    \n",
    "    print(\"\\nImproved NER Results:\")\n",
    "    for result in results:\n",
    "        print(f\"{result['word']} -> {result['entity']} (confidence: {result['score']:.3f})\")\n",
    "    \n",
    "    # Group by entity type\n",
    "    grouped = {}\n",
    "    for result in results:\n",
    "        entity_type = result['entity'].replace('B-', '').replace('I-', '')\n",
    "        if entity_type not in grouped:\n",
    "            grouped[entity_type] = []\n",
    "        grouped[entity_type].append({\n",
    "            'word': result['word'],\n",
    "            'score': result['score']\n",
    "        })\n",
    "    \n",
    "    print(\"\\nGrouped Results:\")\n",
    "    for entity_type, items in grouped.items():\n",
    "        print(f\"{entity_type}: {[item['word'] for item in items]}\")\n",
    "    \n",
    "    # Extract structured information\n",
    "    print(\"\\nExtracted Information:\")\n",
    "    \n",
    "    # Medication name\n",
    "    med_names = grouped.get(\"MEDICATION_NAME\", [])\n",
    "    if med_names:\n",
    "        best_med = max(med_names, key=lambda x: x['score'])\n",
    "        print(f\"Medication: {best_med['word']} (confidence: {best_med['score']:.3f})\")\n",
    "    \n",
    "    # Dosage\n",
    "    dosages = grouped.get(\"DOSAGE\", [])\n",
    "    if dosages:\n",
    "        best_dosage = max(dosages, key=lambda x: x['score'])\n",
    "        print(f\"Dosage: {best_dosage['word']} (confidence: {best_dosage['score']:.3f})\")\n",
    "    \n",
    "    # Instructions\n",
    "    instructions = grouped.get(\"INSTRUCTION\", [])\n",
    "    if instructions:\n",
    "        print(f\"Instructions: {' '.join([inst['word'] for inst in instructions])}\")\n",
    "    \n",
    "    # Notes\n",
    "    notes = grouped.get(\"NOTE\", [])\n",
    "    if notes:\n",
    "        print(f\"Notes: {' '.join([note['word'] for note in notes])}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_current_model_with_improvements()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffe634e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== CONFIG KEYS ==\n",
      "['_name_or_path', 'architectures', 'attention_probs_dropout_prob', 'classifier_dropout', 'gradient_checkpointing', 'hidden_act', 'hidden_dropout_prob', 'hidden_size', 'id2label', 'initializer_range', 'intermediate_size', 'label2id', 'layer_norm_eps', 'max_position_embeddings', 'model_type', 'num_attention_heads', 'num_hidden_layers', 'pad_token_id', 'position_embedding_type', 'torch_dtype'] ...\n",
      "\n",
      "_config.model_type: bert\n",
      "_config.id2label present?: True\n",
      "_config.num_labels present?: False\n",
      "Derived num_labels from id2label: 11\n",
      "\n",
      "Loaded pytorch_model.bin\n",
      "classifier.weight shape: (11, 768)\n",
      "classifier.bias shape: (11,)\n"
     ]
    }
   ],
   "source": [
    "import os, json, torch, pprint\n",
    "\n",
    "p = r\"C:\\Users\\prisc\\OneDrive\\Desktop\\Github\\MediMind\\ml\\new_tesseract_ner\\ner_model5\"\n",
    "\n",
    "# -- Read config.json safely\n",
    "cfg_path = os.path.join(p, \"config.json\")\n",
    "with open(cfg_path, \"r\") as f:\n",
    "    cfg = json.load(f)\n",
    "\n",
    "print(\"== CONFIG KEYS ==\")\n",
    "print(sorted(cfg.keys())[:20], \"...\")\n",
    "print(\"\\n_config.model_type:\", cfg.get(\"model_type\"))\n",
    "print(\"_config.id2label present?:\", \"id2label\" in cfg)\n",
    "print(\"_config.num_labels present?:\", \"num_labels\" in cfg)\n",
    "\n",
    "# Derive label count from id2label if needed\n",
    "derived_num_labels = len(cfg.get(\"id2label\", {})) if \"id2label\" in cfg else None\n",
    "print(\"Derived num_labels from id2label:\", derived_num_labels)\n",
    "\n",
    "# -- Load weights (bin or safetensors)\n",
    "sd = None\n",
    "binp = os.path.join(p, \"pytorch_model.bin\")\n",
    "stp = os.path.join(p, \"model.safetensors\")\n",
    "\n",
    "if os.path.exists(binp):\n",
    "    sd = torch.load(binp, map_location=\"cpu\")\n",
    "    print(\"\\nLoaded pytorch_model.bin\")\n",
    "elif os.path.exists(stp):\n",
    "    from safetensors.torch import load_file\n",
    "    sd = load_file(stp)\n",
    "    print(\"\\nLoaded model.safetensors\")\n",
    "else:\n",
    "    print(\"\\nNo weights file found in:\", p)\n",
    "\n",
    "# -- Print classifier head shapes if weights found\n",
    "if sd is not None:\n",
    "    w_shape = b_shape = None\n",
    "    for k, v in sd.items():\n",
    "        if k.endswith(\"classifier.weight\"):\n",
    "            w_shape = tuple(v.shape)\n",
    "        elif k.endswith(\"classifier.bias\"):\n",
    "            b_shape = tuple(v.shape)\n",
    "    print(\"classifier.weight shape:\", w_shape)\n",
    "    print(\"classifier.bias shape:\", b_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73e5aca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"medicationName\": \"Paracetamol\",\n",
      "    \"intakeQuantity\": 1,\n",
      "    \"frequency\": 2,\n",
      "    \"instructions\": \"after food\",\n",
      "    \"notes\": \"fever pain\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'medicationName': 'Paracetamol',\n",
       " 'intakeQuantity': 1,\n",
       " 'frequency': 2,\n",
       " 'instructions': 'after food',\n",
       " 'notes': 'fever pain'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "# Load NER model\n",
    "model_path = r\"C:\\Users\\prisc\\OneDrive\\Desktop\\Github\\MediMind\\ml\\new_tesseract_ner\\ner_model5\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_path, local_files_only=True)\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"none\")\n",
    "\n",
    "# Converts word-like numbers to numeric\n",
    "def word_to_number(word):\n",
    "    word_map = {\n",
    "        \"one\": 1, \"1\": 1,\n",
    "        \"two\": 2, \"2\": 2,\n",
    "        \"three\": 3, \"3\": 3,\n",
    "        \"four\": 4, \"4\": 4,\n",
    "        \"half\": 0.5, \"quarter\": 0.25\n",
    "    }\n",
    "    return word_map.get(word.lower())\n",
    "\n",
    "# Merges BERT subword tokens (e.g., lo ##rata ##dine ‚Üí loratadine)\n",
    "def merge_subwords(entities):\n",
    "    merged = []\n",
    "    for ent in entities:\n",
    "        if ent[\"word\"].startswith(\"##\") and merged:\n",
    "            merged[-1][\"word\"] += ent[\"word\"][2:]\n",
    "        else:\n",
    "            merged.append(ent.copy())\n",
    "    return merged\n",
    "\n",
    "# Groups entities by label (removes BIO prefix)\n",
    "def group_entities_by_label(entities):\n",
    "    grouped = defaultdict(list)\n",
    "    current_label = None\n",
    "    current_words = []\n",
    "\n",
    "    for ent in entities:\n",
    "        tag = ent[\"entity\"]\n",
    "        if \"-\" in tag:\n",
    "            prefix, label = tag.split(\"-\")\n",
    "        else:\n",
    "            prefix, label = \"O\", tag\n",
    "\n",
    "        if prefix == \"B\":\n",
    "            if current_label and current_words:\n",
    "                grouped[current_label].append(\" \".join(current_words))\n",
    "            current_label = label\n",
    "            current_words = [ent[\"word\"]]\n",
    "        elif prefix == \"I\" and label == current_label:\n",
    "            current_words.append(ent[\"word\"])\n",
    "        else:\n",
    "            if current_label and current_words:\n",
    "                grouped[current_label].append(\" \".join(current_words))\n",
    "            current_label = None\n",
    "            current_words = []\n",
    "\n",
    "            if prefix == \"B\":\n",
    "                current_label = label\n",
    "                current_words = [ent[\"word\"]]\n",
    "\n",
    "    if current_label and current_words:\n",
    "        grouped[current_label].append(\" \".join(current_words))\n",
    "\n",
    "    return grouped\n",
    "\n",
    "# Cleans text like \"##tablet\" to \"tablet\"\n",
    "def clean_text(text):\n",
    "    return text.replace(\" ##\", \"\").replace(\"##\", \"\").strip()\n",
    "\n",
    "# Main function to process input text\n",
    "def infer_and_format(text):\n",
    "    raw_output = ner_pipeline(text)\n",
    "    merged_output = merge_subwords(raw_output)\n",
    "    grouped_output = group_entities_by_label(merged_output)\n",
    "\n",
    "    final = {}\n",
    "\n",
    "    # Medication Name\n",
    "    meds = grouped_output.get(\"MEDICATION_NAME\", [])\n",
    "    if meds:\n",
    "        final[\"medicationName\"] = clean_text(meds[0])\n",
    "\n",
    "    # Dosage\n",
    "    dosages = grouped_output.get(\"DOSAGE\", [])\n",
    "    quantity = 0\n",
    "    for d in dosages:\n",
    "        for word in d.split():\n",
    "            num = word_to_number(word)\n",
    "            if num is not None:\n",
    "                quantity = num\n",
    "                break\n",
    "        if quantity: break\n",
    "    final[\"intakeQuantity\"] = quantity\n",
    "\n",
    "    # Frequency\n",
    "    freqs = grouped_output.get(\"FREQUENCY\", [])\n",
    "    freq_number = 0\n",
    "    for f in freqs:\n",
    "        for word in f.split():\n",
    "            num = word_to_number(word)\n",
    "            if num is not None:\n",
    "                freq_number = num\n",
    "                break\n",
    "        if freq_number: break\n",
    "    final[\"frequency\"] = freq_number\n",
    "\n",
    "    # Instructions\n",
    "    instr = grouped_output.get(\"INSTRUCTION\", [])\n",
    "    if instr:\n",
    "        final[\"instructions\"] = clean_text(\" \".join(instr))\n",
    "\n",
    "    # Notes\n",
    "    notes = grouped_output.get(\"NOTE\", [])\n",
    "    if notes:\n",
    "        final[\"notes\"] = clean_text(\" \".join(notes))\n",
    "\n",
    "    # Save JSON\n",
    "    with open(\"ner_output2.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(final, f, indent=4)\n",
    "\n",
    "    # Print final JSON only\n",
    "    print(json.dumps(final, indent=4))\n",
    "    return final\n",
    "\n",
    "# üîç Run test\n",
    "infer_and_format(\"Paracetamol Take 1 tablet 2 times a day after food for fever and pain relief\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69fe8f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_labels: 11\n",
      "id2label: {0: 'B-DOSAGE', 1: 'I-DOSAGE', 2: 'B-FREQUENCY', 3: 'I-FREQUENCY', 4: 'B-INSTRUCTION', 5: 'I-INSTRUCTION', 6: 'B-MEDICATION_NAME', 7: 'I-MEDICATION_NAME', 8: 'B-NOTE', 9: 'I-NOTE', 10: 'O'}\n",
      "Cleaned: KEEP FluVOXAMine MALEATE 50MG TABLET STEP 3: TAKE HALF TABLET EVERY OTHER NIGHT TAKE WITH OR AFTER FOOD. AVOID ALCOHOL. TEL: 6930 2262\n",
      "\n",
      "Predictions:\n",
      "F -> MEDICATION_NAME (0.506)\n",
      "\n",
      "Grouped:\n",
      "MEDICATION_NAME : ['F']\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, BertTokenizerFast, BertForTokenClassification\n",
    "import re\n",
    "\n",
    "MODEL_PATH = r\"C:\\Users\\prisc\\OneDrive\\Desktop\\Github\\MediMind\\ml\\new_tesseract_ner\\ner_model5\"\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # light OCR normalizations\n",
    "    text = re.sub(r'\\bTAB\\b', 'TABLET', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\bS0MG\\b', '50MG', text, flags=re.IGNORECASE)   # common OCR slip (zero vs O)\n",
    "    text = re.sub(r'\\bSOMG\\b', '50MG', text, flags=re.IGNORECASE)   # your sample had \"SOMG\"\n",
    "    return text\n",
    "\n",
    "# Load EXACTLY as trained (11 labels); no ignore_mismatched_sizes\n",
    "tok = BertTokenizerFast.from_pretrained(MODEL_PATH, local_files_only=True)\n",
    "model = BertForTokenClassification.from_pretrained(MODEL_PATH, local_files_only=True)\n",
    "\n",
    "print(\"num_labels:\", model.num_labels)\n",
    "print(\"id2label:\", model.config.id2label)\n",
    "\n",
    "ner = pipeline(\"ner\", model=model, tokenizer=tok, aggregation_strategy=\"simple\")\n",
    "\n",
    "noisy_text = \"\"\"KEEP FluVOXAMine MALEATE SOMG TAB\n",
    "STEP 3: TAKE HALF TABLET EVERY OTHER NIGHT\n",
    "TAKE WITH OR AFTER FOOD. AVOID ALCOHOL.\n",
    "TEL: 6930 2262\"\"\"\n",
    "\n",
    "cleaned = preprocess_text(noisy_text)\n",
    "print(\"Cleaned:\", cleaned)\n",
    "\n",
    "preds = ner(cleaned)  # [{'entity_group': 'BLAH', 'word': 'xxx', 'score': 0.87, ...}, ...]\n",
    "\n",
    "# Optional confidence filter\n",
    "preds = [p for p in preds if p['score'] >= 0.50]\n",
    "\n",
    "print(\"\\nPredictions:\")\n",
    "for p in preds:\n",
    "    print(f\"{p['word']} -> {p['entity_group']} ({p['score']:.3f})\")\n",
    "\n",
    "# Quick grouping\n",
    "from collections import defaultdict\n",
    "grouped = defaultdict(list)\n",
    "for p in preds:\n",
    "    grouped[p['entity_group']].append(p['word'])\n",
    "\n",
    "print(\"\\nGrouped:\")\n",
    "for k,v in grouped.items():\n",
    "    print(k, \":\", v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85c469ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Raw text ===\n",
      "keep away from children total: 15\n",
      "2 2c-1-112 145 tab (1/1) eee\n",
      "= fluvoxamine maleate somg tab\n",
      "\n",
      "z\n",
      "<\n",
      "3\n",
      "=\n",
      "s\n",
      "=\n",
      "¬∞\n",
      "5\n",
      "\n",
      "step 3: take half tablet every other night\n",
      "take with or after food. avoid alcohol.\n",
      "\n",
      "dionis wee yun ru ¬∞y-skh 4010/2023\n",
      "xxxxx982h\n",
      "mds / soc1-skh-23-1505924001 00003wek\n",
      "\n",
      "sengkang general\n",
      "\n",
      "¬ß at\n",
      "hospital, outpatient pharmacy\n",
      "410 sengkang east way, singapore 644886 tel: 6930 2262\n",
      "\n",
      "=== Cleaned text ===\n",
      "keep away from children total: 15 2 2c-1-112 145 tab (1/1) eee fluvoxamine maleate somg tab z 3 s ¬∞ 5 step 3: take half tablet every other night take with or after food. avoid alcohol. dionis wee yun ru ¬∞y-skh 4010/2023 xxxxx982h mds / soc1-skh-23-1505924001 00003wek sengkang general at hospital, outpatient pharmacy 410 sengkang east way, singapore 644886 tel: 6930 2262\n",
      "\n",
      "Model num_labels: 11\n",
      "Model id2label: {0: 'B-DOSAGE', 1: 'I-DOSAGE', 2: 'B-FREQUENCY', 3: 'I-FREQUENCY', 4: 'B-INSTRUCTION', 5: 'I-INSTRUCTION', 6: 'B-MEDICATION_NAME', 7: 'I-MEDICATION_NAME', 8: 'B-NOTE', 9: 'I-NOTE', 10: 'O'}\n",
      "\n",
      "=== Aggregated predictions ===\n",
      "from children        -> NOTE               (0.459)\n",
      "half tablet          -> DOSAGE             (0.989)\n",
      "every                -> FREQUENCY          (0.993)\n",
      "other night          -> FREQUENCY          (0.755)\n",
      "with or after food   -> INSTRUCTION        (0.981)\n",
      "avoid alcohol        -> NOTE               (0.753)\n",
      "\n",
      "=== Grouped entities (threshold 0.35) ===\n",
      "NOTE              : ['from children', 'avoid alcohol']\n",
      "DOSAGE            : ['half tablet']\n",
      "FREQUENCY         : ['every', 'other night']\n",
      "INSTRUCTION       : ['with or after food']\n",
      "\n",
      "=== Structured ===\n",
      "medication  : \n",
      "dosage      : half tablet\n",
      "frequency   : every other night\n",
      "instruction : with or after food\n",
      "note        : from children avoid alcohol\n",
      "text        : keep away from children total: 15 2 2c-1-112 145 tab (1/1) eee fluvoxamine maleate somg tab z 3 s ¬∞ 5 step 3: take half tablet every other night take with or after food. avoid alcohol. dionis wee yun ru ¬∞y-skh 4010/2023 xxxxx982h mds / soc1-skh-23-1505924001 00003wek sengkang general at hospital, outpatient pharmacy 410 sengkang east way, singapore 644886 tel: 6930 2262\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Preprocess -> 11-label NER (no Tesseract)\n",
    "\n",
    "- Uses your preprocess_ocr_text() exactly (with the tiny regex fix).\n",
    "- Loads model from MODEL_DIR.\n",
    "- Runs NER with aggregation and prints a structured summary.\n",
    "\n",
    "If you want the raw TEST_TEXT lowercased before preprocessing,\n",
    "set LOWERCASE_TEST_TEXT = True.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "from typing import List, Dict, Any\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "# === Configuration ===\n",
    "MODEL_DIR = r\"C:\\Users\\prisc\\OneDrive\\Desktop\\Github\\MediMind\\ml\\new_tesseract_ner\\ner_model5\"\n",
    "CONF_THRESHOLD = 0.35\n",
    "LOWERCASE_TEST_TEXT = True  # <-- toggle to True if you want test text lowercased\n",
    "\n",
    "TEST_TEXT = \"\"\"KEEP AWAY FROM CHILDREN Total: 15\n",
    "2 2C-1-112 145 TAB (1/1) eee\n",
    "= FluVOXAMine MALEATE SOMG TAB\n",
    "\n",
    "z\n",
    "<\n",
    "3\n",
    "=\n",
    "S\n",
    "=\n",
    "¬∞\n",
    "5\n",
    "\n",
    "STEP 3: TAKE HALF TABLET EVERY OTHER NIGHT\n",
    "TAKE WITH OR AFTER FOOD. AVOID ALCOHOL.\n",
    "\n",
    "DIONIS WEE YUN RU ¬∞y-SKH 4010/2023\n",
    "XXXXx982H\n",
    "MDS / SOC1-SKH-23-1505924001 00003WEK\n",
    "\n",
    "SENGKANG GENERAL\n",
    "\n",
    "¬ß at\n",
    "HOSPITAL, Outpatient Pharmacy\n",
    "410 Sengkang East Way, Singapore 644886 TEL: 6930 2262\"\"\"\n",
    " \n",
    "\n",
    "# === Your preprocessing function (kept as-is except for a tiny regex fix) ===\n",
    "def preprocess_ocr_text(text) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # Replace known OCR noise characters\n",
    "    text = text.replace(\"√¢‚Ç¨≈ì\", \"\").replace(\"√¢‚Ç¨\", \"\").replace(\"√¢‚Ç¨Àú\", \"\").replace(\"√¢‚Ç¨‚Ñ¢\", \"'\")\n",
    "\n",
    "    # Remove unwanted symbols\n",
    "    # (Fix: removed a stray empty alternative '||' that could over-match)\n",
    "    text = re.sub(r\"(√¢|√Ç|¬¢|¬ß|¬´|¬©|¬Æ|‚Ç¨|‚Äú|‚Äù|‚Äò|‚Äô|‚Ñ¢|‚Ä¶|_|=|‚Ä¢|‚Äî|‚Äì|@|%|<|>|\\\\|\\||~|`)\", \"\", text)\n",
    "\n",
    "    # Fix common formatting issues\n",
    "    text = re.sub(r\"(\\d)(tab/s|tablet[s]?|cap[s]?|capsule[s]?)\", r\"\\1 tablet\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"(\\d)(times)\", r\"\\1 times\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"(\\d)\\s*x\\s*(a|per)?\\s*day\", r\"\\1 times a day\", text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Normalize known expressions\n",
    "    replacements = {\n",
    "        \"twice a day\": \"2 times a day\",\n",
    "        \"three times daily\": \"3 times a day\",\n",
    "        \"when necessary\": \"when needed\",\n",
    "        \"when required\": \"when needed\",\n",
    "    }\n",
    "    for wrong, correct in replacements.items():\n",
    "        text = re.sub(rf\"\\b{wrong}\\b\", correct, text, flags=re.IGNORECASE)\n",
    "\n",
    "    # remove irrelevant data\n",
    "    lines = text.splitlines()\n",
    "    cleaned = []\n",
    "    for line in lines:\n",
    "        # Skip if line contains clinic/address info\n",
    "        if any(x in line for x in [\"clinic\", \"centre\", \"hospital\", \"#\", \"blk\", \"building\", \"road\", \"s \"]):\n",
    "            continue\n",
    "        # Skip prices and quantities\n",
    "        if re.search(r\"\\bqty\\b|\\bprice\\b|\\$\\d+|\\d+\\.\\d{2}\", line):\n",
    "            continue\n",
    "        cleaned.append(line)\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def group_entities(preds: List[Dict[str, Any]], min_score: float) -> Dict[str, List[str]]:\n",
    "    grouped: Dict[str, List[str]] = {}\n",
    "    for p in preds:\n",
    "        if p.get(\"score\", 0) < min_score:\n",
    "            continue\n",
    "        k = p.get(\"entity_group\") or p.get(\"entity\")\n",
    "        if not k:\n",
    "            continue\n",
    "        grouped.setdefault(k, []).append(p[\"word\"])\n",
    "    return grouped\n",
    "\n",
    "def main():\n",
    "    raw_text = TEST_TEXT.lower() if LOWERCASE_TEST_TEXT else TEST_TEXT\n",
    "    print(\"=== Raw text ===\")\n",
    "    print(raw_text)\n",
    "\n",
    "    cleaned = preprocess_ocr_text(raw_text)\n",
    "    print(\"\\n=== Cleaned text ===\")\n",
    "    print(cleaned)\n",
    "\n",
    "    # Load NER (11 labels)\n",
    "    tok = AutoTokenizer.from_pretrained(MODEL_DIR, local_files_only=True)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(MODEL_DIR, local_files_only=True)\n",
    "    print(\"\\nModel num_labels:\", model.num_labels)\n",
    "    print(\"Model id2label:\", model.config.id2label)\n",
    "\n",
    "    ner = pipeline(\"ner\", model=model, tokenizer=tok, aggregation_strategy=\"simple\")\n",
    "\n",
    "    preds = ner(cleaned)\n",
    "    print(\"\\n=== Aggregated predictions ===\")\n",
    "    for p in preds:\n",
    "        print(f\"{p['word']:20} -> {p['entity_group']:18} ({p['score']:.3f})\")\n",
    "\n",
    "    grouped = group_entities(preds, min_score=CONF_THRESHOLD)\n",
    "    print(\"\\n=== Grouped entities (threshold {:.2f}) ===\".format(CONF_THRESHOLD))\n",
    "    for k, v in grouped.items():\n",
    "        print(f\"{k:18}: {v}\")\n",
    "\n",
    "    # Structured summary\n",
    "    join = lambda xs: \" \".join(xs) if xs else \"\"\n",
    "    summary = {\n",
    "        \"medication\":  join(grouped.get(\"MEDICATION_NAME\", [])),\n",
    "        \"dosage\":      join(grouped.get(\"DOSAGE\", [])),\n",
    "        \"frequency\":   join(grouped.get(\"FREQUENCY\", [])),\n",
    "        \"instruction\": join(grouped.get(\"INSTRUCTION\", [])),\n",
    "        \"note\":        join(grouped.get(\"NOTE\", [])),\n",
    "        \"text\":        cleaned,\n",
    "    }\n",
    "\n",
    "    print(\"\\n=== Structured ===\")\n",
    "    for k, v in summary.items():\n",
    "        print(f\"{k:12}: {v}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d1773e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Raw text ===\n",
      "keep fluvoxamine maleate somg tab\n",
      "step 3: take half tablet every other night\n",
      "take with or after food. avoid alcohol.\n",
      "tel: 6930 2262\n",
      "\n",
      "=== Cleaned text ===\n",
      "keep fluvoxamine maleate somg tab step 3: take half tablet every other night take with or after food. avoid alcohol. tel: 6930 2262\n",
      "\n",
      "Model num_labels: 11\n",
      "Model id2label: {0: 'B-DOSAGE', 1: 'I-DOSAGE', 2: 'B-FREQUENCY', 3: 'I-FREQUENCY', 4: 'B-INSTRUCTION', 5: 'I-INSTRUCTION', 6: 'B-MEDICATION_NAME', 7: 'I-MEDICATION_NAME', 8: 'B-NOTE', 9: 'I-NOTE', 10: 'O'}\n",
      "\n",
      "=== Aggregated predictions ===\n",
      "keep                 -> MEDICATION_NAME    (0.973)\n",
      "flu                  -> MEDICATION_NAME    (0.988)\n",
      "##vo                 -> MEDICATION_NAME    (0.988)\n",
      "##xa                 -> MEDICATION_NAME    (0.995)\n",
      "##mine               -> MEDICATION_NAME    (0.987)\n",
      "##ate                -> MEDICATION_NAME    (0.690)\n",
      "take                 -> NOTE               (0.593)\n",
      "half tablet          -> DOSAGE             (0.985)\n",
      "every                -> FREQUENCY          (0.993)\n",
      "other night          -> FREQUENCY          (0.774)\n",
      "with or after food   -> INSTRUCTION        (0.987)\n",
      "avoid alcohol        -> NOTE               (0.888)\n",
      "\n",
      "=== Grouped entities (threshold 0.35) ===\n",
      "MEDICATION_NAME   : ['keep', 'flu', '##vo', '##xa', '##mine', '##ate']\n",
      "NOTE              : ['take', 'avoid alcohol']\n",
      "DOSAGE            : ['half tablet']\n",
      "FREQUENCY         : ['every', 'other night']\n",
      "INSTRUCTION       : ['with or after food']\n",
      "\n",
      "=== Structured ===\n",
      "medication  : keep flu ##vo ##xa ##mine ##ate\n",
      "dosage      : half tablet\n",
      "frequency   : every other night\n",
      "instruction : with or after food\n",
      "note        : take avoid alcohol\n",
      "text        : keep fluvoxamine maleate somg tab step 3: take half tablet every other night take with or after food. avoid alcohol. tel: 6930 2262\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Preprocess -> 11-label NER (no Tesseract)\n",
    "\n",
    "- Uses your preprocess_ocr_text() exactly (with the tiny regex fix).\n",
    "- Loads model from MODEL_DIR.\n",
    "- Runs NER with aggregation and prints a structured summary.\n",
    "\n",
    "If you want the raw TEST_TEXT lowercased before preprocessing,\n",
    "set LOWERCASE_TEST_TEXT = True.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "from typing import List, Dict, Any\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "# === Configuration ===\n",
    "MODEL_DIR = r\"C:\\Users\\prisc\\OneDrive\\Desktop\\Github\\MediMind\\ml\\new_tesseract_ner\\ner_model5\"\n",
    "CONF_THRESHOLD = 0.35\n",
    "LOWERCASE_TEST_TEXT = True  # <-- toggle to True if you want test text lowercased\n",
    "\n",
    "TEST_TEXT = \"\"\"KEEP FluVOXAMine MALEATE SOMG TAB\n",
    "STEP 3: TAKE HALF TABLET EVERY OTHER NIGHT\n",
    "TAKE WITH OR AFTER FOOD. AVOID ALCOHOL.\n",
    "TEL: 6930 2262\"\"\"\n",
    "\n",
    "# === Your preprocessing function (kept as-is except for a tiny regex fix) ===\n",
    "def preprocess_ocr_text(text) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # Replace known OCR noise characters\n",
    "    text = text.replace(\"√¢‚Ç¨≈ì\", \"\").replace(\"√¢‚Ç¨\", \"\").replace(\"√¢‚Ç¨Àú\", \"\").replace(\"√¢‚Ç¨‚Ñ¢\", \"'\")\n",
    "\n",
    "    # Remove unwanted symbols\n",
    "    # (Fix: removed a stray empty alternative '||' that could over-match)\n",
    "    text = re.sub(r\"(√¢|√Ç|¬¢|¬ß|¬´|¬©|¬Æ|‚Ç¨|‚Äú|‚Äù|‚Äò|‚Äô|‚Ñ¢|‚Ä¶|_|=|‚Ä¢|‚Äî|‚Äì|@|%|<|>|\\\\|\\||~|`)\", \"\", text)\n",
    "\n",
    "    # Fix common formatting issues\n",
    "    text = re.sub(r\"(\\d)(tab/s|tablet[s]?|cap[s]?|capsule[s]?)\", r\"\\1 tablet\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"(\\d)(times)\", r\"\\1 times\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"(\\d)\\s*x\\s*(a|per)?\\s*day\", r\"\\1 times a day\", text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Normalize known expressions\n",
    "    replacements = {\n",
    "        \"twice a day\": \"2 times a day\",\n",
    "        \"three times daily\": \"3 times a day\",\n",
    "        \"when necessary\": \"when needed\",\n",
    "        \"when required\": \"when needed\",\n",
    "    }\n",
    "    for wrong, correct in replacements.items():\n",
    "        text = re.sub(rf\"\\b{wrong}\\b\", correct, text, flags=re.IGNORECASE)\n",
    "\n",
    "    # remove irrelevant data\n",
    "    lines = text.splitlines()\n",
    "    cleaned = []\n",
    "    for line in lines:\n",
    "        # Skip if line contains clinic/address info\n",
    "        if any(x in line for x in [\"clinic\", \"centre\", \"hospital\", \"#\", \"blk\", \"building\", \"road\", \"s \"]):\n",
    "            continue\n",
    "        # Skip prices and quantities\n",
    "        if re.search(r\"\\bqty\\b|\\bprice\\b|\\$\\d+|\\d+\\.\\d{2}\", line):\n",
    "            continue\n",
    "        cleaned.append(line)\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def group_entities(preds: List[Dict[str, Any]], min_score: float) -> Dict[str, List[str]]:\n",
    "    grouped: Dict[str, List[str]] = {}\n",
    "    for p in preds:\n",
    "        if p.get(\"score\", 0) < min_score:\n",
    "            continue\n",
    "        k = p.get(\"entity_group\") or p.get(\"entity\")\n",
    "        if not k:\n",
    "            continue\n",
    "        grouped.setdefault(k, []).append(p[\"word\"])\n",
    "    return grouped\n",
    "\n",
    "def main():\n",
    "    raw_text = TEST_TEXT.lower() if LOWERCASE_TEST_TEXT else TEST_TEXT\n",
    "    print(\"=== Raw text ===\")\n",
    "    print(raw_text)\n",
    "\n",
    "    cleaned = preprocess_ocr_text(raw_text)\n",
    "    print(\"\\n=== Cleaned text ===\")\n",
    "    print(cleaned)\n",
    "\n",
    "    # Load NER (11 labels)\n",
    "    tok = AutoTokenizer.from_pretrained(MODEL_DIR, local_files_only=True)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(MODEL_DIR, local_files_only=True)\n",
    "    print(\"\\nModel num_labels:\", model.num_labels)\n",
    "    print(\"Model id2label:\", model.config.id2label)\n",
    "\n",
    "    ner = pipeline(\"ner\", model=model, tokenizer=tok, aggregation_strategy=\"simple\")\n",
    "\n",
    "    preds = ner(cleaned)\n",
    "    print(\"\\n=== Aggregated predictions ===\")\n",
    "    for p in preds:\n",
    "        print(f\"{p['word']:20} -> {p['entity_group']:18} ({p['score']:.3f})\")\n",
    "\n",
    "    grouped = group_entities(preds, min_score=CONF_THRESHOLD)\n",
    "    print(\"\\n=== Grouped entities (threshold {:.2f}) ===\".format(CONF_THRESHOLD))\n",
    "    for k, v in grouped.items():\n",
    "        print(f\"{k:18}: {v}\")\n",
    "\n",
    "    # Structured summary\n",
    "    join = lambda xs: \" \".join(xs) if xs else \"\"\n",
    "    summary = {\n",
    "        \"medication\":  join(grouped.get(\"MEDICATION_NAME\", [])),\n",
    "        \"dosage\":      join(grouped.get(\"DOSAGE\", [])),\n",
    "        \"frequency\":   join(grouped.get(\"FREQUENCY\", [])),\n",
    "        \"instruction\": join(grouped.get(\"INSTRUCTION\", [])),\n",
    "        \"note\":        join(grouped.get(\"NOTE\", [])),\n",
    "        \"text\":        cleaned,\n",
    "    }\n",
    "\n",
    "    print(\"\\n=== Structured ===\")\n",
    "    for k, v in summary.items():\n",
    "        print(f\"{k:12}: {v}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f2dbf147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== OCR raw ===\n",
      "bebrg fluvvoxamine maleate somg tab\n",
      "step 3: take half tablet every other night\n",
      "take with or after food. avoid alcohol.\n",
      "\n",
      "\n",
      "=== Cleaned text ===\n",
      "bebrg fluvvoxamine maleate somg tab step 3: take half tablet every other night take with or after food. avoid alcohol.\n",
      "\n",
      "Model num_labels: 11\n",
      "Model id2label: {0: 'B-DOSAGE', 1: 'I-DOSAGE', 2: 'B-FREQUENCY', 3: 'I-FREQUENCY', 4: 'B-INSTRUCTION', 5: 'I-INSTRUCTION', 6: 'B-MEDICATION_NAME', 7: 'I-MEDICATION_NAME', 8: 'B-NOTE', 9: 'I-NOTE', 10: 'O'}\n",
      "\n",
      "=== Aggregated predictions ===\n",
      "be                   -> MEDICATION_NAME    (0.991)\n",
      "##b                  -> MEDICATION_NAME    (0.967)\n",
      "##rg                 -> MEDICATION_NAME    (0.961)\n",
      "##mine maleate       -> MEDICATION_NAME    (0.694)\n",
      "take                 -> NOTE               (0.766)\n",
      "half tablet          -> DOSAGE             (0.951)\n",
      "every                -> FREQUENCY          (0.987)\n",
      "other night          -> FREQUENCY          (0.727)\n",
      "with or after food   -> INSTRUCTION        (0.986)\n",
      "avoid alcohol        -> NOTE               (0.824)\n",
      "\n",
      "=== Grouped entities (threshold 0.35) ===\n",
      "MEDICATION_NAME   : ['be', '##b', '##rg', '##mine maleate']\n",
      "NOTE              : ['take', 'avoid alcohol']\n",
      "DOSAGE            : ['half tablet']\n",
      "FREQUENCY         : ['every', 'other night']\n",
      "INSTRUCTION       : ['with or after food']\n",
      "\n",
      "=== Structured ===\n",
      "medication  : be ##b ##rg ##mine maleate\n",
      "dosage      : half tablet\n",
      "frequency   : every other night\n",
      "instruction : with or after food\n",
      "note        : take avoid alcohol\n",
      "ocr_text    : bebrg fluvvoxamine maleate somg tab step 3: take half tablet every other night take with or after food. avoid alcohol.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Image -> Tesseract OCR -> preprocess_ocr_text -> 11-label NER\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "from typing import List, Dict, Any\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "# === Paths / Config ===\n",
    "IMAGE_PATH = r\"C:\\Users\\prisc\\Downloads\\DionisMed.jpeg\"\n",
    "MODEL_DIR  = r\"C:\\Users\\prisc\\OneDrive\\Desktop\\Github\\MediMind\\ml\\new_tesseract_ner\\ner_model5\"\n",
    "\n",
    "TESS_LANG = \"eng\"\n",
    "TESS_CFG  = \"--oem 3 --psm 6\"   # block of text\n",
    "CONF_THRESHOLD = 0.35\n",
    "LOWERCASE_AFTER_OCR = True     # keep False (you said lowercase affects med-name matching)\n",
    "\n",
    "# Try to auto-detect Tesseract on Windows if not on PATH\n",
    "if os.name == \"nt\":\n",
    "    default_tess = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "    if os.path.exists(default_tess):\n",
    "        pytesseract.pytesseract.tesseract_cmd = default_tess\n",
    "\n",
    "# === Your preprocessing function (unchanged except for a tiny regex fix) ===\n",
    "def preprocess_ocr_text(text) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # Replace known OCR noise characters\n",
    "    text = text.replace(\"√¢‚Ç¨≈ì\", \"\").replace(\"√¢‚Ç¨\", \"\").replace(\"√¢‚Ç¨Àú\", \"\").replace(\"√¢‚Ç¨‚Ñ¢\", \"'\")\n",
    "\n",
    "    # Remove unwanted symbols\n",
    "    # (Fix: removed a stray empty alternative '||' that could over-match)\n",
    "    text = re.sub(r\"(√¢|√Ç|¬¢|¬ß|¬´|¬©|¬Æ|‚Ç¨|‚Äú|‚Äù|‚Äò|‚Äô|‚Ñ¢|‚Ä¶|_|=|‚Ä¢|‚Äî|‚Äì|@|%|<|>|\\\\|\\||~|`)\", \"\", text)\n",
    "\n",
    "    # Fix common formatting issues\n",
    "    text = re.sub(r\"(\\d)(tab/s|tablet[s]?|cap[s]?|capsule[s]?)\", r\"\\1 tablet\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"(\\d)(times)\", r\"\\1 times\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"(\\d)\\s*x\\s*(a|per)?\\s*day\", r\"\\1 times a day\", text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Normalize known expressions\n",
    "    replacements = {\n",
    "        \"twice a day\": \"2 times a day\",\n",
    "        \"three times daily\": \"3 times a day\",\n",
    "        \"when necessary\": \"when needed\",\n",
    "        \"when required\": \"when needed\",\n",
    "    }\n",
    "    for wrong, correct in replacements.items():\n",
    "        text = re.sub(rf\"\\b{wrong}\\b\", correct, text, flags=re.IGNORECASE)\n",
    "\n",
    "    # remove irrelevant data\n",
    "    lines = text.splitlines()\n",
    "    cleaned = []\n",
    "    for line in lines:\n",
    "        # Skip if line contains clinic/address info\n",
    "        if any(x in line for x in [\"clinic\", \"centre\", \"hospital\", \"#\", \"blk\", \"building\", \"road\", \"s \"]):\n",
    "            continue\n",
    "        # Skip prices and quantities\n",
    "        if re.search(r\"\\bqty\\b|\\bprice\\b|\\$\\d+|\\d+\\.\\d{2}\", line):\n",
    "            continue\n",
    "        cleaned.append(line)\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def group_entities(preds: List[Dict[str, Any]], min_score: float) -> Dict[str, List[str]]:\n",
    "    grouped: Dict[str, List[str]] = {}\n",
    "    for p in preds:\n",
    "        if p.get(\"score\", 0) < min_score:\n",
    "            continue\n",
    "        k = p.get(\"entity_group\") or p.get(\"entity\")\n",
    "        if not k:\n",
    "            continue\n",
    "        grouped.setdefault(k, []).append(p[\"word\"])\n",
    "    return grouped\n",
    "\n",
    "def main():\n",
    "    # 1) OCR from image\n",
    "    if not os.path.exists(IMAGE_PATH):\n",
    "        raise FileNotFoundError(f\"Image not found: {IMAGE_PATH}\")\n",
    "    img = Image.open(IMAGE_PATH).convert(\"RGB\")\n",
    "    ocr_text = pytesseract.image_to_string(img, lang=TESS_LANG, config=TESS_CFG)\n",
    "    if LOWERCASE_AFTER_OCR:\n",
    "        ocr_text = ocr_text.lower()\n",
    "\n",
    "    print(\"=== OCR raw ===\")\n",
    "    print(ocr_text)\n",
    "\n",
    "    # 2) Preprocess\n",
    "    cleaned = preprocess_ocr_text(ocr_text)\n",
    "    print(\"\\n=== Cleaned text ===\")\n",
    "    print(cleaned)\n",
    "\n",
    "    # 3) Load NER (11 labels) & run\n",
    "    tok = AutoTokenizer.from_pretrained(MODEL_DIR, local_files_only=True)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(MODEL_DIR, local_files_only=True)\n",
    "    print(\"\\nModel num_labels:\", model.num_labels)\n",
    "    print(\"Model id2label:\", model.config.id2label)\n",
    "\n",
    "    ner = pipeline(\"ner\", model=model, tokenizer=tok, aggregation_strategy=\"simple\")\n",
    "    preds = ner(cleaned)\n",
    "\n",
    "    print(\"\\n=== Aggregated predictions ===\")\n",
    "    for p in preds:\n",
    "        print(f\"{p['word']:20} -> {p['entity_group']:18} ({p['score']:.3f})\")\n",
    "\n",
    "    grouped = group_entities(preds, min_score=CONF_THRESHOLD)\n",
    "    print(\"\\n=== Grouped entities (threshold {:.2f}) ===\".format(CONF_THRESHOLD))\n",
    "    for k, v in grouped.items():\n",
    "        print(f\"{k:18}: {v}\")\n",
    "\n",
    "    # 4) Structured summary\n",
    "    join = lambda xs: \" \".join(xs) if xs else \"\"\n",
    "    summary = {\n",
    "        \"medication\":  join(grouped.get(\"MEDICATION_NAME\", [])),\n",
    "        \"dosage\":      join(grouped.get(\"DOSAGE\", [])),\n",
    "        \"frequency\":   join(grouped.get(\"FREQUENCY\", [])),\n",
    "        \"instruction\": join(grouped.get(\"INSTRUCTION\", [])),\n",
    "        \"note\":        join(grouped.get(\"NOTE\", [])),\n",
    "        \"ocr_text\":    cleaned,\n",
    "    }\n",
    "\n",
    "    print(\"\\n=== Structured ===\")\n",
    "    for k, v in summary.items():\n",
    "        print(f\"{k:12}: {v}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0182a2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== OCR raw ===\n",
      "bebrg fluvvoxamine maleate somg tab\n",
      "step 3: take half tablet every other night\n",
      "take with or after food. avoid alcohol.\n",
      "\n",
      "\n",
      "=== Cleaned text ===\n",
      "bebrg fluvvoxamine maleate somg tab step 3: take half tablet every other night take with or after food. avoid alcohol.\n",
      "\n",
      "Model num_labels: 11\n",
      "Model id2label: {0: 'B-DOSAGE', 1: 'I-DOSAGE', 2: 'B-FREQUENCY', 3: 'I-FREQUENCY', 4: 'B-INSTRUCTION', 5: 'I-INSTRUCTION', 6: 'B-MEDICATION_NAME', 7: 'I-MEDICATION_NAME', 8: 'B-NOTE', 9: 'I-NOTE', 10: 'O'}\n",
      "\n",
      "=== Final JSON ===\n",
      "{\n",
      "    \"medicationName\": \"bebrgmine maleate\",\n",
      "    \"intakeQuantity\": 0.5,\n",
      "    \"frequency\": 0,\n",
      "    \"instructions\": \"with or after food\",\n",
      "    \"notes\": \"avoid alcohol\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Image -> Tesseract OCR -> preprocess_ocr_text (UNMODIFIED) -> NER (agg='none')\n",
    "-> merge subwords -> group IOB -> DrugBank fuzzy-correct -> structured JSON\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import difflib\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Any\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "# ======= PATHS / CONFIG =======\n",
    "IMAGE_PATH   = r\"C:\\Users\\prisc\\Downloads\\DionisMed.jpeg\"\n",
    "MODEL_DIR    = r\"C:\\Users\\prisc\\OneDrive\\Desktop\\Github\\MediMind\\ml\\new_tesseract_ner\\ner_model5\"\n",
    "DRUGBANK_CSV = r\"C:\\Users\\prisc\\OneDrive\\Desktop\\Github\\MediMind\\ml\\drugbank_vocabulary.csv\"\n",
    "\n",
    "TESS_LANG = \"eng\"\n",
    "TESS_CFG  = \"--oem 3 --psm 6\"   # block of text\n",
    "LOWERCASE_AFTER_OCR = True      # <-- set False if you don't want lowercasing\n",
    "\n",
    "# Try to auto-detect Tesseract on Windows if not on PATH\n",
    "if os.name == \"nt\":\n",
    "    default_tess = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "    if os.path.exists(default_tess):\n",
    "        pytesseract.pytesseract.tesseract_cmd = default_tess\n",
    "\n",
    "# ======= PREPROCESSING (YOUR ORIGINAL VERSION, UNCHANGED) =======\n",
    "def preprocess_ocr_text(text) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # Replace known OCR noise characters\n",
    "    text = text.replace(\"√¢‚Ç¨≈ì\", \"\").replace(\"√¢‚Ç¨\", \"\").replace(\"√¢‚Ç¨Àú\", \"\").replace(\"√¢‚Ç¨‚Ñ¢\", \"'\")\n",
    "\n",
    "    # Remove unwanted symbols (original pattern preserved)\n",
    "    text = re.sub(r\"(√¢|√Ç|¬¢|¬ß|¬´|¬©|¬Æ|‚Ç¨|‚Äú|‚Äù|‚Äò|‚Äô|‚Ñ¢|‚Ä¶|_|=||‚Ä¢|‚Äî|‚Äì|@|%|<|>|\\\\|\\||~|`)\", \"\", text)\n",
    "\n",
    "    # Fix common formatting issues\n",
    "    text = re.sub(r\"(\\d)(tab/s|tablet[s]?|cap[s]?|capsule[s]?)\", r\"\\1 tablet\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"(\\d)(times)\", r\"\\1 times\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"(\\d)\\s*x\\s*(a|per)?\\s*day\", r\"\\1 times a day\", text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Normalize known expressions\n",
    "    replacements = {\n",
    "        \"twice a day\": \"2 times a day\",\n",
    "        \"three times daily\": \"3 times a day\",\n",
    "        \"when necessary\": \"when needed\",\n",
    "        \"when required\": \"when needed\",\n",
    "    }\n",
    "\n",
    "    for wrong, correct in replacements.items():\n",
    "        text = re.sub(rf\"\\b{wrong}\\b\", correct, text, flags=re.IGNORECASE)\n",
    "\n",
    "    # remove irrelevant data\n",
    "    lines = text.splitlines()\n",
    "    cleaned = []\n",
    "\n",
    "    for line in lines:\n",
    "        # Skip if line contains clinic/address info\n",
    "        if any(x in line for x in [\"clinic\", \"centre\", \"hospital\", \"#\", \"blk\", \"building\", \"road\", \"s \"]):\n",
    "            continue\n",
    "        # Skip prices and quantities\n",
    "        if re.search(r\"\\bqty\\b|\\bprice\\b|\\$\\d+|\\d+\\.\\d{2}\", line):\n",
    "            continue\n",
    "        cleaned.append(line)\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "# ======= POSTPROCESSING (YOURS) =======\n",
    "def load_drugbank_vocab(csv_path, column=\"name\"):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    return df[column].dropna().str.lower().unique().tolist()\n",
    "\n",
    "def correct_drug_name(name, drugbank_vocab):\n",
    "    matches = difflib.get_close_matches(name.lower(), drugbank_vocab, n=1, cutoff=0.7)\n",
    "    return matches[0] if matches else name\n",
    "\n",
    "def word_to_number(word):\n",
    "    word_map = {\n",
    "        \"one\": 1, \"1\": 1,\n",
    "        \"two\": 2, \"2\": 2,\n",
    "        \"three\": 3, \"3\": 3,\n",
    "        \"four\": 4, \"4\": 4,\n",
    "        \"half\": 0.5, \"quarter\": 0.25\n",
    "    }\n",
    "    return word_map.get(word.lower())\n",
    "\n",
    "def merge_subwords(entities):\n",
    "    merged = []\n",
    "    for ent in entities:\n",
    "        if ent[\"word\"].startswith(\"##\") and merged:\n",
    "            merged[-1][\"word\"] += ent[\"word\"][2:]\n",
    "        else:\n",
    "            merged.append(ent.copy())\n",
    "    return merged\n",
    "\n",
    "def group_entities_by_label(entities):\n",
    "    grouped = defaultdict(list)\n",
    "    current_label = None\n",
    "    current_words = []\n",
    "\n",
    "    for ent in entities:\n",
    "        tag = ent[\"entity\"]\n",
    "        prefix, label = tag.split(\"-\") if \"-\" in tag else (\"O\", tag)\n",
    "\n",
    "        if prefix == \"B\":\n",
    "            if current_label and current_words:\n",
    "                grouped[current_label].append(\" \".join(current_words))\n",
    "            current_label = label\n",
    "            current_words = [ent[\"word\"]]\n",
    "        elif prefix == \"I\" and label == current_label:\n",
    "            current_words.append(ent[\"word\"])\n",
    "        else:\n",
    "            if current_label and current_words:\n",
    "                grouped[current_label].append(\" \".join(current_words))\n",
    "            current_label = None\n",
    "            current_words = []\n",
    "\n",
    "            if prefix == \"B\":\n",
    "                current_label = label\n",
    "                current_words = [ent[\"word\"]]\n",
    "\n",
    "    if current_label and current_words:\n",
    "        grouped[current_label].append(\" \".join(current_words))\n",
    "\n",
    "    return grouped\n",
    "\n",
    "def clean_text(text):\n",
    "    return text.replace(\" ##\", \"\").replace(\"##\", \"\").strip()\n",
    "\n",
    "def infer_and_format(text, drugbank_vocab, ner_pipeline):\n",
    "    raw_output = ner_pipeline(text)\n",
    "    merged_output = merge_subwords(raw_output)\n",
    "    grouped_output = group_entities_by_label(merged_output)\n",
    "\n",
    "    final = {}\n",
    "\n",
    "    # Medication Name\n",
    "    meds = grouped_output.get(\"MEDICATION_NAME\", [])\n",
    "    if meds:\n",
    "        med_name = clean_text(meds[0])\n",
    "        corrected = correct_drug_name(med_name, drugbank_vocab)\n",
    "        final[\"medicationName\"] = corrected\n",
    "\n",
    "    # Dosage or Quantity\n",
    "    dosages = grouped_output.get(\"DOSAGE\", [])\n",
    "    quantity = 0\n",
    "    for d in dosages:\n",
    "        for word in d.split():\n",
    "            num = word_to_number(word)\n",
    "            if num is not None:\n",
    "                quantity = num\n",
    "                break\n",
    "        if quantity: break\n",
    "    final[\"intakeQuantity\"] = quantity\n",
    "\n",
    "    # Frequency\n",
    "    freqs = grouped_output.get(\"FREQUENCY\", [])\n",
    "    freq_number = 0\n",
    "    for f in freqs:\n",
    "        for word in f.split():\n",
    "            num = word_to_number(word)\n",
    "            if num is not None:\n",
    "                freq_number = num\n",
    "                break\n",
    "        if freq_number: break\n",
    "    final[\"frequency\"] = freq_number\n",
    "\n",
    "    # Instructions\n",
    "    instr = grouped_output.get(\"INSTRUCTION\", [])\n",
    "    if instr:\n",
    "        final[\"instructions\"] = clean_text(\" \".join(instr))\n",
    "\n",
    "    # Notes\n",
    "    notes = grouped_output.get(\"NOTE\", [])\n",
    "    if notes:\n",
    "        final[\"notes\"] = clean_text(\" \".join(notes))\n",
    "\n",
    "    # Save to JSON\n",
    "    with open(\"ner_v5_output1.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(final, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    return final\n",
    "\n",
    "# ======= MAIN =======\n",
    "def main():\n",
    "    # 1) OCR\n",
    "    if not os.path.exists(IMAGE_PATH):\n",
    "        raise FileNotFoundError(f\"Image not found: {IMAGE_PATH}\")\n",
    "    img = Image.open(IMAGE_PATH).convert(\"RGB\")\n",
    "    ocr_text = pytesseract.image_to_string(img, lang=TESS_LANG, config=TESS_CFG)\n",
    "    if LOWERCASE_AFTER_OCR:\n",
    "        ocr_text = ocr_text.lower()\n",
    "\n",
    "    print(\"=== OCR raw ===\")\n",
    "    print(ocr_text)\n",
    "\n",
    "    # 2) Preprocess (your original function)\n",
    "    cleaned_text = preprocess_ocr_text(ocr_text)\n",
    "    print(\"\\n=== Cleaned text ===\")\n",
    "    print(cleaned_text)\n",
    "\n",
    "    # 3) NER (agg='none' so your merge_subwords + grouping work)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, local_files_only=True)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(MODEL_DIR, local_files_only=True)\n",
    "    print(\"\\nModel num_labels:\", model.num_labels)\n",
    "    print(\"Model id2label:\", model.config.id2label)\n",
    "    ner_pipe = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"none\")\n",
    "\n",
    "    # 4) DrugBank vocab\n",
    "    if not os.path.exists(DRUGBANK_CSV):\n",
    "        raise FileNotFoundError(f\"DrugBank CSV not found: {DRUGBANK_CSV}\")\n",
    "    vocab = load_drugbank_vocab(DRUGBANK_CSV, column=\"name\")\n",
    "\n",
    "    # 5) Inference + postprocessing\n",
    "    final = infer_and_format(cleaned_text, vocab, ner_pipe)\n",
    "\n",
    "    print(\"\\n=== Final JSON ===\")\n",
    "    print(json.dumps(final, indent=4, ensure_ascii=False))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0beeb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== OCR raw ===\n",
      "¬¢\n",
      "rad\n",
      "a\n",
      "&\n",
      "Ms\n",
      "Wy ¬• peer an oat\n",
      "Me hak e\n",
      "‚Äî - hes\n",
      "z KEEP AWAY FROM CHILDREN Total: 157\n",
      "SRA 2C-1-112 45 TAB (1/1) ci aa\n",
      "- = FluVOXAMine MALEATE 50MG TAB\n",
      "| STEP 3: TAKE HALF TABLET EVERY OTHER NIGHT i\n",
      "f TAKE WITH OR AFTER FOOD. AVOID ALCOHOL. :\n",
      ": 4 :\n",
      "DIONIS WEE YUN RU Ap SKH Toi102023\n",
      "XXXXx982H ~ We\n",
      "MDS/ ¬ßOC1-SKH-23-1505924001 OO003WEK 3 NES\n",
      "SENGKANG GENERAL HOSPITAL, Outpatient Pharmacy er\n",
      "410 Sengkang East Way, Singapore 544886 TEL: 6930 2262\n",
      "\n",
      "\n",
      "=== Cleaned text ===\n",
      "rad a & Ms Wy ¬• peer an oat Me hak e - hes z KEEP AWAY FROM CHILDREN Total: 157 SRA 2C-1-112 45 TAB (1/1) ci aa - FluVOXAMine MALEATE 50MG TAB STEP 3: TAKE HALF TABLET EVERY OTHER NIGHT i f TAKE WITH OR AFTER FOOD. AVOID ALCOHOL. : : 4 : DIONIS WEE YUN RU Ap SKH Toi102023 XXXXx982H We MDS/ OC1-SKH-23-1505924001 OO003WEK 3 NES SENGKANG GENERAL HOSPITAL, Outpatient Pharmacy er 410 Sengkang East Way, Singapore 544886 TEL: 6930 2262\n",
      "\n",
      "Model num_labels: 11\n",
      "Model id2label: {0: 'B-DOSAGE', 1: 'I-DOSAGE', 2: 'B-FREQUENCY', 3: 'I-FREQUENCY', 4: 'B-INSTRUCTION', 5: 'I-INSTRUCTION', 6: 'B-MEDICATION_NAME', 7: 'I-MEDICATION_NAME', 8: 'B-NOTE', 9: 'I-NOTE', 10: 'O'}\n",
      "\n",
      "=== Final JSON ===\n",
      "{\n",
      "    \"intakeQuantity\": 0,\n",
      "    \"frequency\": 0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Image -> Tesseract OCR -> preprocess_ocr_text (UNMODIFIED) -> NER (agg='none')\n",
    "-> merge subwords -> group IOB -> DrugBank fuzzy-correct -> structured JSON\n",
    "frequency work but medication name doesnt\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import difflib\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Any\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "# ======= PATHS / CONFIG =======\n",
    "IMAGE_PATH   = r\"C:\\Users\\prisc\\Downloads\\WhatsApp Image 2025-08-07 at 12.38.50 PM.jpeg\"\n",
    "MODEL_DIR    = r\"C:\\Users\\prisc\\OneDrive\\Desktop\\Github\\MediMind\\ml\\new_tesseract_ner\\ner_model5\"\n",
    "DRUGBANK_CSV = r\"C:\\Users\\prisc\\OneDrive\\Desktop\\Github\\MediMind\\ml\\drugbank_vocabulary.csv\"\n",
    "\n",
    "TESS_LANG = \"eng\"\n",
    "TESS_CFG  = \"--oem 3 --psm 6\"   # block of text\n",
    "LOWERCASE_AFTER_OCR = False      # <-- set False if you don't want lowercasing\n",
    "\n",
    "# Try to auto-detect Tesseract on Windows if not on PATH\n",
    "if os.name == \"nt\":\n",
    "    default_tess = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "    if os.path.exists(default_tess):\n",
    "        pytesseract.pytesseract.tesseract_cmd = default_tess\n",
    "\n",
    "# ======= PREPROCESSING (YOUR ORIGINAL VERSION, UNCHANGED) =======\n",
    "def preprocess_ocr_text(text) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # Replace known OCR noise characters\n",
    "    text = text.replace(\"√¢‚Ç¨≈ì\", \"\").replace(\"√¢‚Ç¨\", \"\").replace(\"√¢‚Ç¨Àú\", \"\").replace(\"√¢‚Ç¨‚Ñ¢\", \"'\")\n",
    "\n",
    "    # Remove unwanted symbols (original pattern preserved)\n",
    "    text = re.sub(r\"(√¢|√Ç|¬¢|¬ß|¬´|¬©|¬Æ|‚Ç¨|‚Äú|‚Äù|‚Äò|‚Äô|‚Ñ¢|‚Ä¶|_|=||‚Ä¢|‚Äî|‚Äì|@|%|<|>|\\\\|\\||~|`)\", \"\", text)\n",
    "\n",
    "    # Fix common formatting issues\n",
    "    text = re.sub(r\"(\\d)(tab/s|tablet[s]?|cap[s]?|capsule[s]?)\", r\"\\1 tablet\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"(\\d)(times)\", r\"\\1 times\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"(\\d)\\s*x\\s*(a|per)?\\s*day\", r\"\\1 times a day\", text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Normalize known expressions\n",
    "    replacements = {\n",
    "        \"twice a day\": \"2 times a day\",\n",
    "        \"three times daily\": \"3 times a day\",\n",
    "        \"when necessary\": \"when needed\",\n",
    "        \"when required\": \"when needed\",\n",
    "    }\n",
    "\n",
    "    for wrong, correct in replacements.items():\n",
    "        text = re.sub(rf\"\\b{wrong}\\b\", correct, text, flags=re.IGNORECASE)\n",
    "\n",
    "    # remove irrelevant data\n",
    "    lines = text.splitlines()\n",
    "    cleaned = []\n",
    "\n",
    "    for line in lines:\n",
    "        # Skip if line contains clinic/address info\n",
    "        if any(x in line for x in [\"clinic\", \"centre\", \"hospital\", \"#\", \"blk\", \"building\", \"road\", \"s \"]):\n",
    "            continue\n",
    "        # Skip prices and quantities\n",
    "        if re.search(r\"\\bqty\\b|\\bprice\\b|\\$\\d+|\\d+\\.\\d{2}\", line):\n",
    "            continue\n",
    "        cleaned.append(line)\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "# ======= POSTPROCESSING (YOURS) =======\n",
    "def load_drugbank_vocab(csv_path, column=\"name\"):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    return df[column].dropna().str.lower().unique().tolist()\n",
    "\n",
    "def correct_drug_name(name, drugbank_vocab):\n",
    "    matches = difflib.get_close_matches(name.lower(), drugbank_vocab, n=1, cutoff=0.7)\n",
    "    return matches[0] if matches else name\n",
    "\n",
    "def word_to_number(word):\n",
    "    word_map = {\n",
    "        \"one\": 1, \"1\": 1, \"once\": 1,\n",
    "        \"two\": 2, \"2\": 2, \"twice\": 2,\n",
    "        \"three\": 3, \"3\": 3, \"thrice\": 3,\n",
    "        \"four\": 4, \"4\": 4,\n",
    "        \"half\": 0.5, \"quarter\": 0.25\n",
    "    }\n",
    "    return word_map.get(word.lower())\n",
    "\n",
    "def merge_subwords(entities):\n",
    "    merged = []\n",
    "    for ent in entities:\n",
    "        if ent[\"word\"].startswith(\"##\") and merged:\n",
    "            merged[-1][\"word\"] += ent[\"word\"][2:]\n",
    "        else:\n",
    "            merged.append(ent.copy())\n",
    "    return merged\n",
    "\n",
    "def group_entities_by_label(entities):\n",
    "    grouped = defaultdict(list)\n",
    "    current_label = None\n",
    "    current_words = []\n",
    "\n",
    "    for ent in entities:\n",
    "        tag = ent[\"entity\"]\n",
    "        prefix, label = tag.split(\"-\") if \"-\" in tag else (\"O\", tag)\n",
    "\n",
    "        if prefix == \"B\":\n",
    "            if current_label and current_words:\n",
    "                grouped[current_label].append(\" \".join(current_words))\n",
    "            current_label = label\n",
    "            current_words = [ent[\"word\"]]\n",
    "        elif prefix == \"I\" and label == current_label:\n",
    "            current_words.append(ent[\"word\"])\n",
    "        else:\n",
    "            if current_label and current_words:\n",
    "                grouped[current_label].append(\" \".join(current_words))\n",
    "            current_label = None\n",
    "            current_words = []\n",
    "\n",
    "            if prefix == \"B\":\n",
    "                current_label = label\n",
    "                current_words = [ent[\"word\"]]\n",
    "\n",
    "    if current_label and current_words:\n",
    "        grouped[current_label].append(\" \".join(current_words))\n",
    "\n",
    "    return grouped\n",
    "\n",
    "def clean_text(text):\n",
    "    return text.replace(\" ##\", \"\").replace(\"##\", \"\").strip()\n",
    "\n",
    "def infer_and_format(text, drugbank_vocab, ner_pipeline):\n",
    "    raw_output = ner_pipeline(text)\n",
    "    merged_output = merge_subwords(raw_output)\n",
    "    grouped_output = group_entities_by_label(merged_output)\n",
    "\n",
    "    final = {}\n",
    "\n",
    "    # Medication Name\n",
    "    meds = grouped_output.get(\"MEDICATION_NAME\", [])\n",
    "    if meds:\n",
    "        med_name = clean_text(meds[0])\n",
    "        corrected = correct_drug_name(med_name, drugbank_vocab)\n",
    "        final[\"medicationName\"] = corrected\n",
    "\n",
    "    # === Dosage / intakeQuantity (rule: first numeric else first word-number; default 0)\n",
    "    dosages = grouped_output.get(\"DOSAGE\", [])\n",
    "    quantity = 0\n",
    "    for d in dosages:\n",
    "        for word in d.split():\n",
    "            num = word_to_number(word)\n",
    "            if num is not None:\n",
    "                quantity = num\n",
    "                break\n",
    "        if quantity:\n",
    "            break\n",
    "    final[\"intakeQuantity\"] = quantity\n",
    "\n",
    "    # === Frequency (rule: first numeric/word-number; else 1 if leftover words like 'daily'; else 0)\n",
    "    freq_phrases = grouped_output.get(\"FREQUENCY\", [])\n",
    "    freq_nums = []\n",
    "    freq_words = []\n",
    "\n",
    "    for phrase in freq_phrases:\n",
    "        for w in phrase.split():\n",
    "            lw = w.lower()\n",
    "            if lw in {\"times\", \"time\"}:\n",
    "                continue  # skip filler words\n",
    "            if w.isnumeric():\n",
    "                freq_nums.append(int(w))\n",
    "            else:\n",
    "                val = word_to_number(w)\n",
    "                if val is not None:\n",
    "                    freq_nums.append(val)\n",
    "                else:\n",
    "                    freq_words.append(w)\n",
    "\n",
    "    if freq_nums:\n",
    "        final[\"frequency\"] = freq_nums[0]\n",
    "    elif freq_words:\n",
    "        final[\"frequency\"] = 1\n",
    "    else:\n",
    "        final[\"frequency\"] = 0\n",
    "\n",
    "    # === Instructions (rule: frequency leftover words + INSTRUCTION tokens)\n",
    "    instr_tokens = freq_words + grouped_output.get(\"INSTRUCTION\", [])\n",
    "    if instr_tokens:\n",
    "        final[\"instructions\"] = clean_text(\" \".join(instr_tokens))\n",
    "\n",
    "    # Notes\n",
    "    notes = grouped_output.get(\"NOTE\", [])\n",
    "    if notes:\n",
    "        final[\"notes\"] = clean_text(\" \".join(notes))\n",
    "\n",
    "    # Save to JSON\n",
    "    with open(\"ner_v5_output1.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(final, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    return final\n",
    "\n",
    "\n",
    "# ======= MAIN =======\n",
    "def main():\n",
    "    # 1) OCR\n",
    "    if not os.path.exists(IMAGE_PATH):\n",
    "        raise FileNotFoundError(f\"Image not found: {IMAGE_PATH}\")\n",
    "    img = Image.open(IMAGE_PATH).convert(\"RGB\")\n",
    "    ocr_text = pytesseract.image_to_string(img, lang=TESS_LANG, config=TESS_CFG)\n",
    "    if LOWERCASE_AFTER_OCR:\n",
    "        ocr_text = ocr_text.lower()\n",
    "\n",
    "    print(\"=== OCR raw ===\")\n",
    "    print(ocr_text)\n",
    "\n",
    "    # 2) Preprocess (your original function)\n",
    "    cleaned_text = preprocess_ocr_text(ocr_text)\n",
    "    print(\"\\n=== Cleaned text ===\")\n",
    "    print(cleaned_text)\n",
    "\n",
    "    # 3) NER (agg='none' so your merge_subwords + grouping work)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, local_files_only=True)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(MODEL_DIR, local_files_only=True)\n",
    "    print(\"\\nModel num_labels:\", model.num_labels)\n",
    "    print(\"Model id2label:\", model.config.id2label)\n",
    "    ner_pipe = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"none\")\n",
    "\n",
    "    # 4) DrugBank vocab\n",
    "    if not os.path.exists(DRUGBANK_CSV):\n",
    "        raise FileNotFoundError(f\"DrugBank CSV not found: {DRUGBANK_CSV}\")\n",
    "    vocab = load_drugbank_vocab(DRUGBANK_CSV, column=\"name\")\n",
    "\n",
    "    # 5) Inference + postprocessing\n",
    "    final = infer_and_format(cleaned_text, vocab, ner_pipe)\n",
    "\n",
    "    print(\"\\n=== Final JSON ===\")\n",
    "    print(json.dumps(final, indent=4, ensure_ascii=False))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433e258c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aa5921ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== OCR raw ===\n",
      "20/20 TA mes 5 > .\n",
      "loRATAdine 10MG TAB 4 | i}\n",
      "TAKE 1 TABLET(S) EVERY MORNING WHEN NEEDED ay\n",
      "May be taken with or without food. May affect alertness Warning - avoid \\ f Nie\n",
      "alcoholic drinks c } |\n",
      "Poy )¬•\n",
      "\n",
      "XxxXx548 28/07/2025 KAL-390030\n",
      "LEWIS HUANG KAIZHEN Lia\n",
      "\n",
      "x c LINIC P J Yi\n",
      " ERREE , <AuE porcine marae B07\n",
      "\n",
      "a : y 4\n",
      "\n",
      "4\n",
      ";\n",
      "\n",
      "\") \\\n",
      "Lage Zs a | |e\n",
      "\n",
      "\n",
      "=== Cleaned text ===\n",
      "20 20 TA mes 5 loRATAdine 10MG TAB 4 i TAKE 1 TABLET S EVERY MORNING WHEN NEEDED ay May be taken with or without food May affect alertness Warning avoid f Nie alcoholic drinks c Poy XxxXx548 28 07 2025 KAL 390030 LEWIS HUANG KAIZHEN Lia x c LINIC P J Yi ERREE AuE porcine marae B07 a y 4 4 Lage Zs a e\n",
      "\n",
      "Model num_labels: 11\n",
      "Model id2label: {0: 'B-DOSAGE', 1: 'I-DOSAGE', 2: 'B-FREQUENCY', 3: 'I-FREQUENCY', 4: 'B-INSTRUCTION', 5: 'I-INSTRUCTION', 6: 'B-MEDICATION_NAME', 7: 'I-MEDICATION_NAME', 8: 'B-NOTE', 9: 'I-NOTE', 10: 'O'}\n",
      "\n",
      "=== Final JSON ===\n",
      "{\n",
      "    \"medicationName\": \"loratadine\",\n",
      "    \"intakeQuantity\": 1,\n",
      "    \"frequency\": 1,\n",
      "    \"instructions\": \"every morning when needed with or without food\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Image -> Tesseract OCR -> preprocess_ocr_text (UNMODIFIED) -> NER (agg='none')\n",
    "-> merge subwords -> group IOB -> DrugBank fuzzy-correct -> structured JSON\n",
    "is in the current OCR pipeline v5 in final model. added preprocessing of Strict filter. added drug name extraction, fuzzy etc\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import difflib\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Any\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "# ======= PATHS / CONFIG =======\n",
    "IMAGE_PATH   = r\"C:\\Users\\prisc\\Downloads\\WhatsApp Image 2025-08-13 at 1.09.17 AM.jpeg\"\n",
    "MODEL_DIR    = r\"C:\\Users\\prisc\\OneDrive\\Desktop\\Github\\MediMind\\ml\\new_tesseract_ner\\ner_model5\"\n",
    "DRUGBANK_CSV = r\"C:\\Users\\prisc\\OneDrive\\Desktop\\Final_ML_model\\drugbank_vocabulary.csv\"\n",
    "\n",
    "TESS_LANG = \"eng\"\n",
    "TESS_CFG  = \"--oem 3 --psm 6\"   # block of text\n",
    "LOWERCASE_AFTER_OCR = False      # <-- set False if you don't want lowercasing\n",
    "\n",
    "# Try to auto-detect Tesseract on Windows if not on PATH\n",
    "if os.name == \"nt\":\n",
    "    default_tess = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "    if os.path.exists(default_tess):\n",
    "        pytesseract.pytesseract.tesseract_cmd = default_tess\n",
    "\n",
    "# ======= PREPROCESSING (YOUR ORIGINAL VERSION, UNCHANGED) =======\n",
    "def preprocess_ocr_text(text) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # Replace known OCR noise characters\n",
    "    text = text.replace(\"√¢‚Ç¨≈ì\", \"\").replace(\"√¢‚Ç¨\", \"\").replace(\"√¢‚Ç¨Àú\", \"\").replace(\"√¢‚Ç¨‚Ñ¢\", \"'\")\n",
    "\n",
    "    # Remove unwanted symbols (original pattern preserved)\n",
    "    text = re.sub(r\"(√¢|√Ç|¬¢|¬ß|¬´|¬©|¬Æ|‚Ç¨|‚Äú|‚Äù|‚Äò|‚Äô|‚Ñ¢|‚Ä¶|_|=||‚Ä¢|‚Äî|‚Äì|@|%|<|>|\\\\|\\||~|`)\", \"\", text)\n",
    "\n",
    "    # Fix common formatting issues\n",
    "    text = re.sub(r\"(\\d)(tab/s|tablet[s]?|cap[s]?|capsule[s]?)\", r\"\\1 tablet\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"(\\d)(times)\", r\"\\1 times\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"(\\d)\\s*x\\s*(a|per)?\\s*day\", r\"\\1 times a day\", text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Normalize known expressions\n",
    "    replacements = {\n",
    "        \"when necessary\": \"when needed\",\n",
    "        \"when required\": \"when needed\",\n",
    "    }\n",
    "    for wrong, correct in replacements.items():\n",
    "        text = re.sub(rf\"\\b{wrong}\\b\", correct, text, flags=re.IGNORECASE)\n",
    "\n",
    "    # remove irrelevant data\n",
    "    lines = text.splitlines()\n",
    "    cleaned = []\n",
    "    for line in lines:\n",
    "        # Skip if line contains clinic/address info\n",
    "        if any(x in line for x in [\"clinic\", \"centre\", \"hospital\", \"#\", \"blk\", \"building\", \"road\", \"s \"]):\n",
    "            continue\n",
    "        # Skip prices and quantities\n",
    "        if re.search(r\"\\bqty\\b|\\bprice\\b|\\$\\d+|\\d+\\.\\d{2}\", line):\n",
    "            continue\n",
    "        cleaned.append(line)\n",
    "\n",
    "    # Strict filter: keep only letters, numbers, and spaces\n",
    "    text = re.sub(r\"[^A-Za-z0-9\\s]\", \" \", text)\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "# ======= POSTPROCESSING (YOURS) =======\n",
    "def load_drugbank_vocab(csv_path, column=\"name\"):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    return df[column].dropna().str.lower().unique().tolist()\n",
    "\n",
    "def correct_drug_name(name, drugbank_vocab):\n",
    "    matches = difflib.get_close_matches(name.lower(), drugbank_vocab, n=1, cutoff=0.7)\n",
    "    return matches[0] if matches else name\n",
    "\n",
    "def is_all_upper_words(text: str) -> bool:\n",
    "    \"\"\"True if there is at least one alphabetic token and all such tokens are UPPERCASE.\"\"\"\n",
    "    tokens = re.findall(r\"[A-Za-z]+\", text)\n",
    "    if not tokens:\n",
    "        return False\n",
    "    return all(t.isupper() for t in tokens)\n",
    "\n",
    "def correct_drug_name_caseaware(name: str, drugbank_vocab, cutoff: float = 0.8):\n",
    "    \"\"\"\n",
    "    Return (best_name, matched_bool). Prefer exact case-insensitive match to DrugBank,\n",
    "    else fuzzy match at >= cutoff, else return original name.\n",
    "    \"\"\"\n",
    "    # Build a map for exact case-insensitive lookup\n",
    "    lower_to_cased = {v.lower(): v for v in drugbank_vocab}\n",
    "\n",
    "    # Exact (case-insensitive)\n",
    "    lower = name.lower()\n",
    "    if lower in lower_to_cased:\n",
    "        return lower_to_cased[lower], True\n",
    "\n",
    "    # Fuzzy (>= cutoff)\n",
    "    cand = difflib.get_close_matches(lower, list(lower_to_cased.keys()), n=1, cutoff=cutoff)\n",
    "    if cand:\n",
    "        return lower_to_cased[cand[0]], True\n",
    "\n",
    "    # No good match: keep original\n",
    "    return name, False\n",
    "\n",
    "def word_to_number(word):\n",
    "    word_map = {\n",
    "        \"one\": 1, \"1\": 1, \"once\": 1,\n",
    "        \"two\": 2, \"2\": 2, \"twice\": 2,\n",
    "        \"three\": 3, \"3\": 3, \"thrice\": 3,\n",
    "        \"four\": 4, \"4\": 4,\n",
    "        \"half\": 0.5, \"quarter\": 0.25\n",
    "    }\n",
    "    return word_map.get(word.lower())\n",
    "\n",
    "def merge_subwords(entities):\n",
    "    merged = []\n",
    "    for ent in entities:\n",
    "        if ent[\"word\"].startswith(\"##\") and merged:\n",
    "            merged[-1][\"word\"] += ent[\"word\"][2:]\n",
    "        else:\n",
    "            merged.append(ent.copy())\n",
    "    return merged\n",
    "\n",
    "def group_entities_by_label(entities):\n",
    "    grouped = defaultdict(list)\n",
    "    current_label = None\n",
    "    current_words = []\n",
    "\n",
    "    for ent in entities:\n",
    "        tag = ent[\"entity\"]\n",
    "        prefix, label = tag.split(\"-\") if \"-\" in tag else (\"O\", tag)\n",
    "\n",
    "        if prefix == \"B\":\n",
    "            if current_label and current_words:\n",
    "                grouped[current_label].append(\" \".join(current_words))\n",
    "            current_label = label\n",
    "            current_words = [ent[\"word\"]]\n",
    "        elif prefix == \"I\" and label == current_label:\n",
    "            current_words.append(ent[\"word\"])\n",
    "        else:\n",
    "            if current_label and current_words:\n",
    "                grouped[current_label].append(\" \".join(current_words))\n",
    "            current_label = None\n",
    "            current_words = []\n",
    "\n",
    "            if prefix == \"B\":\n",
    "                current_label = label\n",
    "                current_words = [ent[\"word\"]]\n",
    "\n",
    "    if current_label and current_words:\n",
    "        grouped[current_label].append(\" \".join(current_words))\n",
    "\n",
    "    return grouped\n",
    "\n",
    "def clean_text(text):\n",
    "    return text.replace(\" ##\", \"\").replace(\"##\", \"\").strip()\n",
    "\n",
    "def find_medication_in_text(text: str, drugbank_vocab, cutoff: float = 0.8):\n",
    "    \"\"\"\n",
    "    Try to find a medication name directly from text using DrugBank vocab.\n",
    "    1) Exact (case-insensitive) substring match (prefer the longest).\n",
    "    2) If none, fuzzy match on 1-3 word windows with >= cutoff.\n",
    "    Return best_name or None.\n",
    "    \"\"\"\n",
    "    txt = text.lower()\n",
    "\n",
    "    # 1) Exact substring matches (prefer the longest)\n",
    "    best_exact = None\n",
    "    for name in drugbank_vocab:  # already lowercased list from load_drugbank_vocab\n",
    "        if name in txt:\n",
    "            if best_exact is None or len(name) > len(best_exact):\n",
    "                best_exact = name\n",
    "    if best_exact:\n",
    "        return best_exact\n",
    "\n",
    "    # 2) Fuzzy on 1-3 word windows\n",
    "    tokens = re.findall(r\"[a-z][a-z\\-]+\", txt)\n",
    "    seen = set()\n",
    "    for i in range(len(tokens)):\n",
    "        for n in (3, 2, 1):  # try longer windows first\n",
    "            if i + n <= len(tokens):\n",
    "                cand = \" \".join(tokens[i:i+n])\n",
    "                if cand in seen:\n",
    "                    continue\n",
    "                seen.add(cand)\n",
    "                match = difflib.get_close_matches(cand, drugbank_vocab, n=1, cutoff=cutoff)\n",
    "                if match:\n",
    "                    return match[0]\n",
    "    return None\n",
    "\n",
    "def canonical_drugbank_spelling(name_lc: str, drugbank_vocab):\n",
    "    \"\"\"Map a lowercased hit back to DrugBank‚Äôs cased spelling if available.\"\"\"\n",
    "    # If your CSV has canonical casing in a separate column, adapt this.\n",
    "    # Here we just title-case as a simple presentation fix.\n",
    "    return name_lc  # keep lowercased; or do: name_lc.title()\n",
    "\n",
    "\n",
    "def infer_and_format(text, drugbank_vocab, ner_pipeline):\n",
    "    raw_output = ner_pipeline(text)\n",
    "    merged_output = merge_subwords(raw_output)\n",
    "    grouped_output = group_entities_by_label(merged_output)\n",
    "\n",
    "    final = {}\n",
    "\n",
    "    # --- Medication Name ---\n",
    "    meds = grouped_output.get(\"MEDICATION_NAME\", [])\n",
    "    if meds:\n",
    "        # Model provided a name -> use your existing correction behavior\n",
    "        med_name = clean_text(meds[0])\n",
    "        corrected, matched = correct_drug_name_caseaware(med_name, drugbank_vocab, cutoff=0.8)\n",
    "        final[\"medicationName\"] = corrected if matched else med_name\n",
    "    else:\n",
    "        # Fallback: detect directly from the cleaned text (the same string you passed in)\n",
    "        fallback = find_medication_in_text(text, drugbank_vocab, cutoff=0.8)\n",
    "        if fallback:\n",
    "            final[\"medicationName\"] = canonical_drugbank_spelling(fallback, drugbank_vocab)\n",
    "\n",
    "\n",
    "    # === Dosage / intakeQuantity (rule: first numeric else first word-number; default 0)\n",
    "    dosages = grouped_output.get(\"DOSAGE\", [])\n",
    "    quantity = 0\n",
    "    for d in dosages:\n",
    "        for word in d.split():\n",
    "            num = word_to_number(word)\n",
    "            if num is not None:\n",
    "                quantity = num\n",
    "                break\n",
    "        if quantity:\n",
    "            break\n",
    "    final[\"intakeQuantity\"] = quantity\n",
    "\n",
    "    # === Frequency (rule: first numeric/word-number; else 1 if leftover words like 'daily'; else 0)\n",
    "    freq_phrases = grouped_output.get(\"FREQUENCY\", [])\n",
    "    freq_nums = []\n",
    "    freq_words = []\n",
    "\n",
    "    for phrase in freq_phrases:\n",
    "        for w in phrase.split():\n",
    "            lw = w.lower()\n",
    "            if lw in {\"times\", \"time\"}:\n",
    "                continue  # skip filler words\n",
    "            if w.isnumeric():\n",
    "                freq_nums.append(int(w))\n",
    "            else:\n",
    "                val = word_to_number(w)\n",
    "                if val is not None:\n",
    "                    freq_nums.append(val)\n",
    "                else:\n",
    "                    freq_words.append(w)\n",
    "\n",
    "    if freq_nums:\n",
    "        final[\"frequency\"] = freq_nums[0]\n",
    "    elif freq_words:\n",
    "        final[\"frequency\"] = 1\n",
    "    else:\n",
    "        final[\"frequency\"] = 0\n",
    "\n",
    "    # === Instructions (rule: frequency leftover words + INSTRUCTION tokens)\n",
    "    instr_tokens = freq_words + grouped_output.get(\"INSTRUCTION\", [])\n",
    "    if instr_tokens:\n",
    "        final[\"instructions\"] = clean_text(\" \".join(instr_tokens))\n",
    "\n",
    "    # Notes\n",
    "    notes = grouped_output.get(\"NOTE\", [])\n",
    "    if notes:\n",
    "        final[\"notes\"] = clean_text(\" \".join(notes))\n",
    "\n",
    "    # Save to JSON\n",
    "    with open(\"ner_v5_output1.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(final, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    return final\n",
    "\n",
    "\n",
    "# ======= MAIN =======\n",
    "def main():\n",
    "    # 1) OCR\n",
    "    if not os.path.exists(IMAGE_PATH):\n",
    "        raise FileNotFoundError(f\"Image not found: {IMAGE_PATH}\")\n",
    "    img = Image.open(IMAGE_PATH).convert(\"RGB\")\n",
    "    ocr_text = pytesseract.image_to_string(img, lang=TESS_LANG, config=TESS_CFG)\n",
    "\n",
    "    # New rule: only lowercase if ALL words are uppercase\n",
    "    if is_all_upper_words(ocr_text):\n",
    "        ocr_text = ocr_text.lower()\n",
    "\n",
    "\n",
    "    print(\"=== OCR raw ===\")\n",
    "    print(ocr_text)\n",
    "\n",
    "    # 2) Preprocess (your original function)\n",
    "    cleaned_text = preprocess_ocr_text(ocr_text)\n",
    "    print(\"\\n=== Cleaned text ===\")\n",
    "    print(cleaned_text)\n",
    "\n",
    "    # 3) NER (agg='none' so your merge_subwords + grouping work)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, local_files_only=True)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(MODEL_DIR, local_files_only=True)\n",
    "    print(\"\\nModel num_labels:\", model.num_labels)\n",
    "    print(\"Model id2label:\", model.config.id2label)\n",
    "    ner_pipe = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"none\")\n",
    "\n",
    "    # 4) DrugBank vocab\n",
    "    if not os.path.exists(DRUGBANK_CSV):\n",
    "        raise FileNotFoundError(f\"DrugBank CSV not found: {DRUGBANK_CSV}\")\n",
    "    vocab = load_drugbank_vocab(DRUGBANK_CSV, column=\"name\")\n",
    "\n",
    "    # 5) Inference + postprocessing\n",
    "    # LOWERCASE ONLY FOR THE NER MODEL INPUT\n",
    "    text_for_ner = cleaned_text.lower()\n",
    "    final = infer_and_format(text_for_ner, vocab, ner_pipe)\n",
    "\n",
    "\n",
    "    print(\"\\n=== Final JSON ===\")\n",
    "    print(json.dumps(final, indent=4, ensure_ascii=False))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c55694e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0813 01:52:55.057000 31140 torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "C:\\Users\\prisc\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\utils\\generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "C:\\Users\\prisc\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\utils\\generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== OCR raw ===\n",
      "¬¢\n",
      "rad\n",
      "a\n",
      "&\n",
      "Ms\n",
      "Wy ¬• peer an oat\n",
      "Me hak e\n",
      "‚Äî - hes\n",
      "z KEEP AWAY FROM CHILDREN Total: 157\n",
      "SRA 2C-1-112 45 TAB (1/1) ci aa\n",
      "- = FluVOXAMine MALEATE 50MG TAB\n",
      "| STEP 3: TAKE HALF TABLET EVERY OTHER NIGHT i\n",
      "f TAKE WITH OR AFTER FOOD. AVOID ALCOHOL. :\n",
      ": 4 :\n",
      "DIONIS WEE YUN RU Ap SKH Toi102023\n",
      "XXXXx982H ~ We\n",
      "MDS/ ¬ßOC1-SKH-23-1505924001 OO003WEK 3 NES\n",
      "SENGKANG GENERAL HOSPITAL, Outpatient Pharmacy er\n",
      "410 Sengkang East Way, Singapore 544886 TEL: 6930 2262\n",
      "\n",
      "\n",
      "=== Cleaned text ===\n",
      "rad a Ms Wy peer an oat Me hak e hes z KEEP AWAY FROM CHILDREN Total 157 SRA 2C 1 112 45 TAB 1 1 ci aa FluVOXAMine MALEATE 50MG TAB STEP 3 TAKE HALF TABLET EVERY OTHER NIGHT i f TAKE WITH OR AFTER FOOD AVOID ALCOHOL 4 DIONIS WEE YUN RU Ap SKH Toi102023 XXXXx982H We MDS OC1 SKH 23 1505924001 OO003WEK 3 NES SENGKANG GENERAL HOSPITAL Outpatient Pharmacy er 410 Sengkang East Way Singapore 544886 TEL 6930 2262\n",
      "\n",
      "Model num_labels: 11\n",
      "Model id2label: {0: 'B-DOSAGE', 1: 'I-DOSAGE', 2: 'B-FREQUENCY', 3: 'I-FREQUENCY', 4: 'B-INSTRUCTION', 5: 'I-INSTRUCTION', 6: 'B-MEDICATION_NAME', 7: 'I-MEDICATION_NAME', 8: 'B-NOTE', 9: 'I-NOTE', 10: 'O'}\n",
      "\n",
      "=== Final JSON ===\n",
      "{\n",
      "    \"medicationName\": \"fluvoxamine\",\n",
      "    \"intakeQuantity\": 0.5,\n",
      "    \"frequency\": 1,\n",
      "    \"instructions\": \"every other night with or after food\",\n",
      "    \"notes\": \"avoid alcohol\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#code inside final_ner_model ocr_ner_pipeline which is the code cleaned from the top codes. This is the submitted one\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import difflib\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Any\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "IMAGE_PATH   = r\"C:\\Users\\prisc\\Downloads\\WhatsApp Image 2025-08-07 at 12.38.50 PM.jpeg\"\n",
    "MODEL_DIR    = r\"C:\\Users\\prisc\\OneDrive\\Desktop\\Github\\MediMind\\ml\\new_tesseract_ner\\ner_model5\"\n",
    "DRUGBANK_CSV = r\"C:\\Users\\prisc\\OneDrive\\Desktop\\Final_ML_model\\drugbank_vocabulary.csv\"\n",
    "\n",
    "TESS_LANG = \"eng\"\n",
    "TESS_CFG  = \"--oem 3 --psm 6\"   \n",
    "LOWERCASE_AFTER_OCR = False   \n",
    "\n",
    "# Try to auto-detect Tesseract on Windows if not on PATH\n",
    "if os.name == \"nt\":\n",
    "    default_tess = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "    if os.path.exists(default_tess):\n",
    "        pytesseract.pytesseract.tesseract_cmd = default_tess\n",
    "\n",
    "# preprocessing\n",
    "def preprocess_ocr_text(text) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # Replace known OCR noise characters\n",
    "    text = text.replace(\"√¢‚Ç¨≈ì\", \"\").replace(\"√¢‚Ç¨\", \"\").replace(\"√¢‚Ç¨Àú\", \"\").replace(\"√¢‚Ç¨‚Ñ¢\", \"'\")\n",
    "\n",
    "    # Remove unwanted symbols (original pattern preserved)\n",
    "    text = re.sub(r\"(√¢|√Ç|¬¢|¬ß|¬´|¬©|¬Æ|‚Ç¨|‚Äú|‚Äù|‚Äò|‚Äô|‚Ñ¢|‚Ä¶|_|=||‚Ä¢|‚Äî|‚Äì|@|%|<|>|\\\\|\\||~|`)\", \"\", text)\n",
    "\n",
    "    # Fix common formatting issues\n",
    "    text = re.sub(r\"(\\d)(tab/s|tablet[s]?|cap[s]?|capsule[s]?)\", r\"\\1 tablet\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"(\\d)(times)\", r\"\\1 times\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"(\\d)\\s*x\\s*(a|per)?\\s*day\", r\"\\1 times a day\", text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Normalize known expressions\n",
    "    replacements = {\n",
    "        \"when necessary\": \"when needed\",\n",
    "        \"when required\": \"when needed\",\n",
    "    }\n",
    "    for wrong, correct in replacements.items():\n",
    "        text = re.sub(rf\"\\b{wrong}\\b\", correct, text, flags=re.IGNORECASE)\n",
    "\n",
    "    # remove irrelevant data\n",
    "    lines = text.splitlines()\n",
    "    cleaned = []\n",
    "    for line in lines:\n",
    "        # Skip if line contains clinic/address info\n",
    "        if any(x in line for x in [\"clinic\", \"centre\", \"hospital\", \"#\", \"blk\", \"building\", \"road\", \"s \"]):\n",
    "            continue\n",
    "        # Skip prices and quantities\n",
    "        if re.search(r\"\\bqty\\b|\\bprice\\b|\\$\\d+|\\d+\\.\\d{2}\", line):\n",
    "            continue\n",
    "        cleaned.append(line)\n",
    "\n",
    "    # keep only letters, numbers, and spaces\n",
    "    text = re.sub(r\"[^A-Za-z0-9\\s]\", \" \", text)\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "# postprocessing\n",
    "def load_drugbank_vocab(csv_path, column=\"name\"):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    return df[column].dropna().str.lower().unique().tolist()\n",
    "\n",
    "def is_all_upper_words(text: str) -> bool:\n",
    "    \"\"\"True if there is at least one alphabetic token and all such tokens are UPPERCASE.\"\"\"\n",
    "    tokens = re.findall(r\"[A-Za-z]+\", text)\n",
    "    if not tokens:\n",
    "        return False\n",
    "    return all(t.isupper() for t in tokens)\n",
    "\n",
    "# case-insensitive. match to DrugBank else match >=cutoff, else return original name\n",
    "def correct_drug_name_caseaware(name: str, drugbank_vocab, cutoff: float = 0.8):\n",
    "    lower_to_cased = {v.lower(): v for v in drugbank_vocab}\n",
    "\n",
    "    lower = name.lower()\n",
    "    if lower in lower_to_cased:\n",
    "        return lower_to_cased[lower], True\n",
    "\n",
    "    cand = difflib.get_close_matches(lower, list(lower_to_cased.keys()), n=1, cutoff=cutoff)\n",
    "    if cand:\n",
    "        return lower_to_cased[cand[0]], True\n",
    "\n",
    "    return name, False\n",
    "\n",
    "def word_to_number(word):\n",
    "    word_map = {\n",
    "        \"one\": 1, \"1\": 1, \"once\": 1,\n",
    "        \"two\": 2, \"2\": 2, \"twice\": 2,\n",
    "        \"three\": 3, \"3\": 3, \"thrice\": 3,\n",
    "        \"four\": 4, \"4\": 4,\n",
    "        \"half\": 0.5, \"quarter\": 0.25\n",
    "    }\n",
    "    return word_map.get(word.lower())\n",
    "\n",
    "def merge_subwords(entities):\n",
    "    merged = []\n",
    "    for ent in entities:\n",
    "        if ent[\"word\"].startswith(\"##\") and merged:\n",
    "            merged[-1][\"word\"] += ent[\"word\"][2:]\n",
    "        else:\n",
    "            merged.append(ent.copy())\n",
    "    return merged\n",
    "\n",
    "def group_entities_by_label(entities):\n",
    "    grouped = defaultdict(list)\n",
    "    current_label = None\n",
    "    current_words = []\n",
    "\n",
    "    for ent in entities:\n",
    "        tag = ent[\"entity\"]\n",
    "        prefix, label = tag.split(\"-\") if \"-\" in tag else (\"O\", tag)\n",
    "\n",
    "        if prefix == \"B\":\n",
    "            if current_label and current_words:\n",
    "                grouped[current_label].append(\" \".join(current_words))\n",
    "            current_label = label\n",
    "            current_words = [ent[\"word\"]]\n",
    "        elif prefix == \"I\" and label == current_label:\n",
    "            current_words.append(ent[\"word\"])\n",
    "        else:\n",
    "            if current_label and current_words:\n",
    "                grouped[current_label].append(\" \".join(current_words))\n",
    "            current_label = None\n",
    "            current_words = []\n",
    "\n",
    "            if prefix == \"B\":\n",
    "                current_label = label\n",
    "                current_words = [ent[\"word\"]]\n",
    "\n",
    "    if current_label and current_words:\n",
    "        grouped[current_label].append(\" \".join(current_words))\n",
    "\n",
    "    return grouped\n",
    "\n",
    "def clean_text(text):\n",
    "    return text.replace(\" ##\", \"\").replace(\"##\", \"\").strip()\n",
    "\n",
    "# To find medication name directly from text using DrugBank vocab.\n",
    "def find_medication_in_text(text: str, drugbank_vocab, cutoff: float = 0.8):\n",
    "   \n",
    "    txt = text.lower()\n",
    "\n",
    "    # 1) Exact substring matches (prefer the longest)\n",
    "    best_exact = None\n",
    "    for name in drugbank_vocab: \n",
    "        if name in txt:\n",
    "            if best_exact is None or len(name) > len(best_exact):\n",
    "                best_exact = name\n",
    "    if best_exact:\n",
    "        return best_exact\n",
    "\n",
    "    # 2) Fuzzy on 1-3 word windows\n",
    "    tokens = re.findall(r\"[a-z][a-z\\-]+\", txt)\n",
    "    seen = set()\n",
    "    for i in range(len(tokens)):\n",
    "        for n in (3, 2, 1):  # try longer windows first\n",
    "            if i + n <= len(tokens):\n",
    "                cand = \" \".join(tokens[i:i+n])\n",
    "                if cand in seen:\n",
    "                    continue\n",
    "                seen.add(cand)\n",
    "                match = difflib.get_close_matches(cand, drugbank_vocab, n=1, cutoff=cutoff)\n",
    "                if match:\n",
    "                    return match[0]\n",
    "    return None\n",
    "\n",
    "\n",
    "def infer_and_format(text, drugbank_vocab, ner_pipeline):\n",
    "    raw_output = ner_pipeline(text)\n",
    "    merged_output = merge_subwords(raw_output)\n",
    "    grouped_output = group_entities_by_label(merged_output)\n",
    "\n",
    "    final = {}\n",
    "\n",
    "    meds = grouped_output.get(\"MEDICATION_NAME\", [])\n",
    "    if meds:\n",
    "        \n",
    "        med_name = clean_text(meds[0])\n",
    "        corrected, matched = correct_drug_name_caseaware(med_name, drugbank_vocab, cutoff=0.8)\n",
    "        final[\"medicationName\"] = corrected if matched else med_name\n",
    "    else:\n",
    "        fallback = find_medication_in_text(text, drugbank_vocab, cutoff=0.8)\n",
    "        if fallback:\n",
    "            final[\"medicationName\"] = fallback\n",
    "\n",
    "\n",
    "    # Dosage/intakeQuantity(first numeric else first word-number; default 0)\n",
    "    dosages = grouped_output.get(\"DOSAGE\", [])\n",
    "    quantity = 0\n",
    "    for d in dosages:\n",
    "        for word in d.split():\n",
    "            num = word_to_number(word)\n",
    "            if num is not None:\n",
    "                quantity = num\n",
    "                break\n",
    "        if quantity:\n",
    "            break\n",
    "    final[\"intakeQuantity\"] = quantity\n",
    "\n",
    "    # Frequency(first numeric/word-number; else 1 if leftover words like 'daily'; else 0)\n",
    "    freq_phrases = grouped_output.get(\"FREQUENCY\", [])\n",
    "    freq_nums = []\n",
    "    freq_words = []\n",
    "\n",
    "    for phrase in freq_phrases:\n",
    "        for w in phrase.split():\n",
    "            lw = w.lower()\n",
    "            if lw in {\"times\", \"time\"}:\n",
    "                continue \n",
    "            if w.isnumeric():\n",
    "                freq_nums.append(int(w))\n",
    "            else:\n",
    "                val = word_to_number(w)\n",
    "                if val is not None:\n",
    "                    freq_nums.append(val)\n",
    "                else:\n",
    "                    freq_words.append(w)\n",
    "\n",
    "    if freq_nums:\n",
    "        final[\"frequency\"] = freq_nums[0]\n",
    "    elif freq_words:\n",
    "        final[\"frequency\"] = 1\n",
    "    else:\n",
    "        final[\"frequency\"] = 0\n",
    "\n",
    "    # Instructions (frequency leftover words + INSTRUCTION tokens)\n",
    "    instr_tokens = freq_words + grouped_output.get(\"INSTRUCTION\", [])\n",
    "    if instr_tokens:\n",
    "        final[\"instructions\"] = clean_text(\" \".join(instr_tokens))\n",
    "\n",
    "    # Notes\n",
    "    notes = grouped_output.get(\"NOTE\", [])\n",
    "    if notes:\n",
    "        final[\"notes\"] = clean_text(\" \".join(notes))\n",
    "\n",
    "    # Save to JSON\n",
    "    with open(\"ner_v5_output1.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(final, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    return final\n",
    "\n",
    "\n",
    "def main():\n",
    "    # OCR\n",
    "    if not os.path.exists(IMAGE_PATH):\n",
    "        raise FileNotFoundError(f\"Image not found: {IMAGE_PATH}\")\n",
    "    img = Image.open(IMAGE_PATH).convert(\"RGB\")\n",
    "    ocr_text = pytesseract.image_to_string(img, lang=TESS_LANG, config=TESS_CFG)\n",
    "\n",
    "    # only lowercase if ALL words are uppercase\n",
    "    if is_all_upper_words(ocr_text):\n",
    "        ocr_text = ocr_text.lower()\n",
    "\n",
    "\n",
    "    print(\"=== OCR raw ===\")\n",
    "    print(ocr_text)\n",
    "\n",
    "    # Preprocess \n",
    "    cleaned_text = preprocess_ocr_text(ocr_text)\n",
    "    print(\"\\n=== Cleaned text ===\")\n",
    "    print(cleaned_text)\n",
    "\n",
    "    # NER \n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, local_files_only=True)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(MODEL_DIR, local_files_only=True)\n",
    "    print(\"\\nModel num_labels:\", model.num_labels)\n",
    "    print(\"Model id2label:\", model.config.id2label)\n",
    "    ner_pipe = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"none\")\n",
    "\n",
    "    # DrugBank vocab\n",
    "    if not os.path.exists(DRUGBANK_CSV):\n",
    "        raise FileNotFoundError(f\"DrugBank CSV not found: {DRUGBANK_CSV}\")\n",
    "    vocab = load_drugbank_vocab(DRUGBANK_CSV, column=\"name\")\n",
    "\n",
    "    # Inference + postprocessing\n",
    "    # LOWERCASE ONLY FOR THE NER MODEL INPUT\n",
    "    text_for_ner = cleaned_text.lower()\n",
    "    final = infer_and_format(text_for_ner, vocab, ner_pipe)\n",
    "\n",
    "\n",
    "    print(\"\\n=== Final JSON ===\")\n",
    "    print(json.dumps(final, indent=4, ensure_ascii=False))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faba925d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== OCR raw ===\n",
      ". r r =, Ar nn \" ‚Äî ee meee f\n",
      "Melatonin 2mg PR Tab (CIRCADIN) Mi\n",
      "\n",
      "se! Take ONE tablet(s) at bedtime ra\n",
      "Take 1 to 2 hours before sleep. Swallow whole. Do not cut, crush or chew. 17\n",
      "ful May cause drowsiness or dizziness. Do not drive or operate machinery. 1 7\n",
      "23 i | Wi\n",
      "Avoid alcohol. ‚Äî ae | yy)\n",
      "\n",
      "am\n",
      "|\n",
      ". i)\n",
      "\n",
      "\n",
      "=== Cleaned text ===\n",
      "r r Ar nn ee meee f Melatonin 2mg PR Tab CIRCADIN Mi se Take ONE tablet s at bedtime ra Take 1 to 2 hours before sleep Swallow whole Do not cut crush or chew 17 ful May cause drowsiness or dizziness Do not drive or operate machinery 1 7 23 i Wi Avoid alcohol ae yy am i\n",
      "\n",
      "Model num_labels: 11\n",
      "Model id2label: {0: 'B-DOSAGE', 1: 'I-DOSAGE', 2: 'B-FREQUENCY', 3: 'I-FREQUENCY', 4: 'B-INSTRUCTION', 5: 'I-INSTRUCTION', 6: 'B-MEDICATION_NAME', 7: 'I-MEDICATION_NAME', 8: 'B-NOTE', 9: 'I-NOTE', 10: 'O'}\n",
      "\n",
      "=== Final JSON ===\n",
      "{\n",
      "    \"medicationName\": \"melatonin\",\n",
      "    \"intakeQuantity\": 1,\n",
      "    \"frequency\": 1,\n",
      "    \"instructions\": \"at bedtime swallow whole do not cut crush or chew\",\n",
      "    \"notes\": \"drowsiness dizziness alcohol\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Image -> Tesseract OCR -> preprocess_ocr_text (UNMODIFIED) -> NER (agg='none')\n",
    "-> merge subwords -> group IOB -> DrugBank fuzzy-correct -> structured JSON\n",
    "frequency work but medication name doesnt\n",
    "\n",
    "Added more medication name reading function but end up is slower so not using\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import difflib\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Any\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "# ======= PATHS / CONFIG =======\n",
    "IMAGE_PATH   = r\"C:\\Users\\prisc\\Downloads\\WhatsApp Image 2025-08-12 at 5.52.20 PM.jpeg\"\n",
    "MODEL_DIR    = r\"C:\\Users\\prisc\\OneDrive\\Desktop\\Github\\MediMind\\ml\\new_tesseract_ner\\ner_model5\"\n",
    "DRUGBANK_CSV = r\"C:\\Users\\prisc\\OneDrive\\Desktop\\Github\\MediMind\\ml\\drugbank_vocabulary.csv\"\n",
    "\n",
    "TESS_LANG = \"eng\"\n",
    "TESS_CFG  = \"--oem 3 --psm 6\"   # block of text\n",
    "LOWERCASE_AFTER_OCR = False      # <-- set False if you don't want lowercasing\n",
    "\n",
    "# Try to auto-detect Tesseract on Windows if not on PATH\n",
    "if os.name == \"nt\":\n",
    "    default_tess = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "    if os.path.exists(default_tess):\n",
    "        pytesseract.pytesseract.tesseract_cmd = default_tess\n",
    "\n",
    "# ======= PREPROCESSING (YOUR ORIGINAL VERSION, UNCHANGED) =======\n",
    "def preprocess_ocr_text(text) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # Replace known OCR noise characters\n",
    "    text = text.replace(\"√¢‚Ç¨≈ì\", \"\").replace(\"√¢‚Ç¨\", \"\").replace(\"√¢‚Ç¨Àú\", \"\").replace(\"√¢‚Ç¨‚Ñ¢\", \"'\")\n",
    "\n",
    "    # Remove unwanted symbols (original pattern preserved)\n",
    "    text = re.sub(r\"(√¢|√Ç|¬¢|¬ß|¬´|¬©|¬Æ|‚Ç¨|‚Äú|‚Äù|‚Äò|‚Äô|‚Ñ¢|‚Ä¶|_|=||‚Ä¢|‚Äî|‚Äì|@|%|<|>|\\\\|\\||~|`)\", \"\", text)\n",
    "\n",
    "    # Fix common formatting issues\n",
    "    text = re.sub(r\"(\\d)(tab/s|tablet[s]?|cap[s]?|capsule[s]?)\", r\"\\1 tablet\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"(\\d)(times)\", r\"\\1 times\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"(\\d)\\s*x\\s*(a|per)?\\s*day\", r\"\\1 times a day\", text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Normalize known expressions\n",
    "    replacements = {\n",
    "        \"twice a day\": \"2 times a day\",\n",
    "        \"three times daily\": \"3 times a day\",\n",
    "        \"when necessary\": \"when needed\",\n",
    "        \"when required\": \"when needed\",\n",
    "    }\n",
    "    for wrong, correct in replacements.items():\n",
    "        text = re.sub(rf\"\\b{wrong}\\b\", correct, text, flags=re.IGNORECASE)\n",
    "\n",
    "    # remove irrelevant data\n",
    "    lines = text.splitlines()\n",
    "    cleaned = []\n",
    "    for line in lines:\n",
    "        # Skip if line contains clinic/address info\n",
    "        if any(x in line for x in [\"clinic\", \"centre\", \"hospital\", \"#\", \"blk\", \"building\", \"road\", \"s \"]):\n",
    "            continue\n",
    "        # Skip prices and quantities\n",
    "        if re.search(r\"\\bqty\\b|\\bprice\\b|\\$\\d+|\\d+\\.\\d{2}\", line):\n",
    "            continue\n",
    "        cleaned.append(line)\n",
    "\n",
    "    # Strict filter: keep only letters, numbers, and spaces\n",
    "    text = re.sub(r\"[^A-Za-z0-9\\s]\", \" \", text)\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "# ======= POSTPROCESSING (YOURS) =======\n",
    "def load_drugbank_vocab(csv_path, column=\"name\"):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    return df[column].dropna().str.lower().unique().tolist()\n",
    "\n",
    "def correct_drug_name(name, drugbank_vocab):\n",
    "    matches = difflib.get_close_matches(name.lower(), drugbank_vocab, n=1, cutoff=0.7)\n",
    "    return matches[0] if matches else name\n",
    "\n",
    "def is_all_upper_words(text: str) -> bool:\n",
    "    \"\"\"True if there is at least one alphabetic token and all such tokens are UPPERCASE.\"\"\"\n",
    "    tokens = re.findall(r\"[A-Za-z]+\", text)\n",
    "    if not tokens:\n",
    "        return False\n",
    "    return all(t.isupper() for t in tokens)\n",
    "\n",
    "def correct_drug_name_caseaware(name: str, drugbank_vocab, cutoff: float = 0.8):\n",
    "    \"\"\"\n",
    "    Return (best_name, matched_bool). Prefer exact case-insensitive match to DrugBank,\n",
    "    else fuzzy match at >= cutoff, else return original name.\n",
    "    \"\"\"\n",
    "    # Build a map for exact case-insensitive lookup\n",
    "    lower_to_cased = {v.lower(): v for v in drugbank_vocab}\n",
    "\n",
    "    # Exact (case-insensitive)\n",
    "    lower = name.lower()\n",
    "    if lower in lower_to_cased:\n",
    "        return lower_to_cased[lower], True\n",
    "\n",
    "    # Fuzzy (>= cutoff)\n",
    "    cand = difflib.get_close_matches(lower, list(lower_to_cased.keys()), n=1, cutoff=cutoff)\n",
    "    if cand:\n",
    "        return lower_to_cased[cand[0]], True\n",
    "\n",
    "    # No good match: keep original\n",
    "    return name, False\n",
    "\n",
    "def word_to_number(word):\n",
    "    word_map = {\n",
    "        \"one\": 1, \"1\": 1, \"once\": 1,\n",
    "        \"two\": 2, \"2\": 2, \"twice\": 2,\n",
    "        \"three\": 3, \"3\": 3, \"thrice\": 3,\n",
    "        \"four\": 4, \"4\": 4,\n",
    "        \"half\": 0.5, \"quarter\": 0.25\n",
    "    }\n",
    "    return word_map.get(word.lower())\n",
    "\n",
    "def merge_subwords(entities):\n",
    "    merged = []\n",
    "    for ent in entities:\n",
    "        if ent[\"word\"].startswith(\"##\") and merged:\n",
    "            merged[-1][\"word\"] += ent[\"word\"][2:]\n",
    "        else:\n",
    "            merged.append(ent.copy())\n",
    "    return merged\n",
    "\n",
    "def group_entities_by_label(entities):\n",
    "    grouped = defaultdict(list)\n",
    "    current_label = None\n",
    "    current_words = []\n",
    "\n",
    "    for ent in entities:\n",
    "        tag = ent[\"entity\"]\n",
    "        prefix, label = tag.split(\"-\") if \"-\" in tag else (\"O\", tag)\n",
    "\n",
    "        if prefix == \"B\":\n",
    "            if current_label and current_words:\n",
    "                grouped[current_label].append(\" \".join(current_words))\n",
    "            current_label = label\n",
    "            current_words = [ent[\"word\"]]\n",
    "        elif prefix == \"I\" and label == current_label:\n",
    "            current_words.append(ent[\"word\"])\n",
    "        else:\n",
    "            if current_label and current_words:\n",
    "                grouped[current_label].append(\" \".join(current_words))\n",
    "            current_label = None\n",
    "            current_words = []\n",
    "\n",
    "            if prefix == \"B\":\n",
    "                current_label = label\n",
    "                current_words = [ent[\"word\"]]\n",
    "\n",
    "    if current_label and current_words:\n",
    "        grouped[current_label].append(\" \".join(current_words))\n",
    "\n",
    "    return grouped\n",
    "\n",
    "def clean_text(text):\n",
    "    return text.replace(\" ##\", \"\").replace(\"##\", \"\").strip()\n",
    "\n",
    "# Precompile once at top of file (optional but nice)\n",
    "_WORD_RE = re.compile(r\"[a-z]+(?:-[a-z]+)*\")\n",
    "\n",
    "def find_medication_in_text(text: str, drugbank_vocab, cutoff: float = 0.8):\n",
    "    \"\"\"\n",
    "    Find a medication name from free text using your DrugBank vocab (lowercased list).\n",
    "    Strategy:\n",
    "      1) Exact whole-phrase (case-insensitive) match using word boundaries; prefer the longest match.\n",
    "      2) Fuzzy match (>= cutoff) over 1‚Äì3 word n-grams; try longer windows first.\n",
    "    Returns: lowercased DrugBank name or None.\n",
    "    \"\"\"\n",
    "    txt = text.lower()\n",
    "\n",
    "    # 1) Exact whole-phrase match (prefer the longest)\n",
    "    best_exact = None\n",
    "    for name in drugbank_vocab:  # drugbank_vocab is already lowercased per your loader\n",
    "        # word-boundary match to avoid partial hits inside other words\n",
    "        if re.search(r\"\\b\" + re.escape(name) + r\"\\b\", txt):\n",
    "            if best_exact is None or len(name) > len(best_exact):\n",
    "                best_exact = name\n",
    "    if best_exact:\n",
    "        return best_exact\n",
    "\n",
    "    # 2) Fuzzy on 1‚Äì3 word windows (longer first)\n",
    "    tokens = _WORD_RE.findall(txt)\n",
    "    for n in (3, 2, 1):\n",
    "        # sliding window\n",
    "        for i in range(0, max(0, len(tokens) - n + 1)):\n",
    "            cand = \" \".join(tokens[i:i + n])\n",
    "            match = difflib.get_close_matches(cand, drugbank_vocab, n=1, cutoff=cutoff)\n",
    "            if match:\n",
    "                return match[0]\n",
    "\n",
    "    return None\n",
    "\n",
    "def canonical_drugbank_spelling(name_lc: str, drugbank_vocab, cased_map: dict | None = None) -> str:\n",
    "    \"\"\"\n",
    "    Map a lowercased DrugBank hit back to canonical casing if you have it.\n",
    "    - If you pass a dict like {\"melatonin\": \"Melatonin\"}, we‚Äôll use that.\n",
    "    - Otherwise we keep the lowercased form.\n",
    "    \"\"\"\n",
    "    if cased_map and name_lc in cased_map:\n",
    "        return cased_map[name_lc]\n",
    "    return name_lc\n",
    "\n",
    "def choose_medication_name(med_candidate: str, text: str, drugbank_vocab, cutoff: float = 0.8):\n",
    "    \"\"\"\n",
    "    Ensure medicationName comes from DrugBank:\n",
    "      1) Try model span ‚Üí exact/fuzzy via correct_drug_name_caseaware.\n",
    "      2) If not matched, scan full text (exact then fuzzy).\n",
    "      3) If still nothing, keep model span only if it looks like a real drug word (>=3 letters).\n",
    "    Returns a string (may be \"\").\n",
    "    \"\"\"\n",
    "    med_candidate = clean_text((med_candidate or \"\")).strip()\n",
    "\n",
    "    if med_candidate:\n",
    "        corrected, matched = correct_drug_name_caseaware(med_candidate, drugbank_vocab, cutoff=cutoff)\n",
    "        if matched:\n",
    "            return corrected  # standardized to DrugBank\n",
    "\n",
    "    # Fallback: pull from the whole text\n",
    "    fb = find_medication_in_text(text, drugbank_vocab, cutoff=cutoff)\n",
    "    if fb:\n",
    "        return canonical_drugbank_spelling(fb, drugbank_vocab)\n",
    "\n",
    "    # Last resort: accept only if it looks plausible (>=3 letters)\n",
    "    return med_candidate if looks_like_valid_drug(med_candidate) else \"\"\n",
    "\n",
    "\n",
    "def infer_and_format(text, drugbank_vocab, ner_pipeline):\n",
    "    raw_output = ner_pipeline(text)\n",
    "    merged_output = merge_subwords(raw_output)\n",
    "    grouped_output = group_entities_by_label(merged_output)\n",
    "\n",
    "    final = {}\n",
    "\n",
    "    # --- Medication Name ---\n",
    "    # Medication Name (exact or >=80% fuzzy to DrugBank; keep original if no good match)\n",
    "    # Medication Name ‚Äî must be from DrugBank, otherwise guess\n",
    "    meds = grouped_output.get(\"MEDICATION_NAME\", [])\n",
    "    med_candidate = meds[0] if meds else \"\"\n",
    "    final_name = choose_medication_name(med_candidate, text, drugbank_vocab, cutoff=0.8)\n",
    "    if final_name:\n",
    "        final[\"medicationName\"] = final_name\n",
    "    else:\n",
    "        # Fallback: detect directly from the cleaned text (the same string you passed in)\n",
    "        fallback = find_medication_in_text(text, drugbank_vocab, cutoff=0.8)\n",
    "        if fallback:\n",
    "            final[\"medicationName\"] = canonical_drugbank_spelling(fallback, drugbank_vocab)\n",
    "\n",
    "\n",
    "    # === Dosage / intakeQuantity (rule: first numeric else first word-number; default 0)\n",
    "    dosages = grouped_output.get(\"DOSAGE\", [])\n",
    "    quantity = 0\n",
    "    for d in dosages:\n",
    "        for word in d.split():\n",
    "            num = word_to_number(word)\n",
    "            if num is not None:\n",
    "                quantity = num\n",
    "                break\n",
    "        if quantity:\n",
    "            break\n",
    "    final[\"intakeQuantity\"] = quantity\n",
    "\n",
    "    # === Frequency (rule: first numeric/word-number; else 1 if leftover words like 'daily'; else 0)\n",
    "    freq_phrases = grouped_output.get(\"FREQUENCY\", [])\n",
    "    freq_nums = []\n",
    "    freq_words = []\n",
    "\n",
    "    for phrase in freq_phrases:\n",
    "        for w in phrase.split():\n",
    "            lw = w.lower()\n",
    "            if lw in {\"times\", \"time\"}:\n",
    "                continue  # skip filler words\n",
    "            if w.isnumeric():\n",
    "                freq_nums.append(int(w))\n",
    "            else:\n",
    "                val = word_to_number(w)\n",
    "                if val is not None:\n",
    "                    freq_nums.append(val)\n",
    "                else:\n",
    "                    freq_words.append(w)\n",
    "\n",
    "    if freq_nums:\n",
    "        final[\"frequency\"] = freq_nums[0]\n",
    "    elif freq_words:\n",
    "        final[\"frequency\"] = 1\n",
    "    else:\n",
    "        final[\"frequency\"] = 0\n",
    "\n",
    "    # === Instructions (rule: frequency leftover words + INSTRUCTION tokens)\n",
    "    instr_tokens = freq_words + grouped_output.get(\"INSTRUCTION\", [])\n",
    "    if instr_tokens:\n",
    "        final[\"instructions\"] = clean_text(\" \".join(instr_tokens))\n",
    "\n",
    "    # Notes\n",
    "    notes = grouped_output.get(\"NOTE\", [])\n",
    "    if notes:\n",
    "        final[\"notes\"] = clean_text(\" \".join(notes))\n",
    "\n",
    "    # Save to JSON\n",
    "    with open(\"ner_v5_output1.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(final, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    return final\n",
    "\n",
    "\n",
    "# ======= MAIN =======\n",
    "def main():\n",
    "    # 1) OCR\n",
    "    if not os.path.exists(IMAGE_PATH):\n",
    "        raise FileNotFoundError(f\"Image not found: {IMAGE_PATH}\")\n",
    "    img = Image.open(IMAGE_PATH).convert(\"RGB\")\n",
    "    ocr_text = pytesseract.image_to_string(img, lang=TESS_LANG, config=TESS_CFG)\n",
    "\n",
    "    # New rule: only lowercase if ALL words are uppercase\n",
    "    if is_all_upper_words(ocr_text):\n",
    "        ocr_text = ocr_text.lower()\n",
    "\n",
    "\n",
    "    print(\"=== OCR raw ===\")\n",
    "    print(ocr_text)\n",
    "\n",
    "    # 2) Preprocess (your original function)\n",
    "    cleaned_text = preprocess_ocr_text(ocr_text)\n",
    "    print(\"\\n=== Cleaned text ===\")\n",
    "    print(cleaned_text)\n",
    "\n",
    "    # 3) NER (agg='none' so your merge_subwords + grouping work)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, local_files_only=True)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(MODEL_DIR, local_files_only=True)\n",
    "    print(\"\\nModel num_labels:\", model.num_labels)\n",
    "    print(\"Model id2label:\", model.config.id2label)\n",
    "    ner_pipe = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"none\")\n",
    "\n",
    "    # 4) DrugBank vocab\n",
    "    if not os.path.exists(DRUGBANK_CSV):\n",
    "        raise FileNotFoundError(f\"DrugBank CSV not found: {DRUGBANK_CSV}\")\n",
    "    vocab = load_drugbank_vocab(DRUGBANK_CSV, column=\"name\")\n",
    "\n",
    "    # 5) Inference + postprocessing\n",
    "    # LOWERCASE ONLY FOR THE NER MODEL INPUT\n",
    "    text_for_ner = cleaned_text.lower()\n",
    "    final = infer_and_format(text_for_ner, vocab, ner_pipe)\n",
    "\n",
    "\n",
    "    print(\"\\n=== Final JSON ===\")\n",
    "    print(json.dumps(final, indent=4, ensure_ascii=False))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "labelstudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
