{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55fd671e",
   "metadata": {},
   "source": [
    "Converted medication label images to text and using similar process as below. Output is clean_clean.json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e381898a",
   "metadata": {},
   "source": [
    "Preprocessing function to clean manually keyed in 50 of the medication label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57fbfdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def preprocess_ocr_text(text) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # Replace known OCR noise characters\n",
    "    text = text.replace(\"â€œ\", \"\").replace(\"â€\", \"\").replace(\"â€˜\", \"\").replace(\"â€™\", \"'\")\n",
    "\n",
    "    # Remove unwanted symbols\n",
    "    text = re.sub(r\"(â|Â|¢|§|«|©|®|€|“|”|‘|’|™|…|_|=||•|—|–|@|%|<|>|\\\\|\\||~|`)\", \"\", text)\n",
    "\n",
    "    # Fix common formatting issues\n",
    "    text = re.sub(r\"(\\d)(tab/s|tablet[s]?|cap[s]?|capsule[s]?)\", r\"\\1 tablet\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"(\\d)(times)\", r\"\\1 times\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"(\\d)\\s*x\\s*(a|per)?\\s*day\", r\"\\1 times a day\", text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Normalize known expressions\n",
    "    replacements = {\n",
    "        \"twice a day\": \"2 times a day\",\n",
    "        \"three times daily\": \"3 times a day\",\n",
    "        \"when necessary\": \"when needed\",\n",
    "        \"when required\": \"when needed\",\n",
    "    }\n",
    "\n",
    "    for wrong, correct in replacements.items():\n",
    "        text = re.sub(rf\"\\b{wrong}\\b\", correct, text, flags=re.IGNORECASE)\n",
    "\n",
    "    #remove irrelevant data\n",
    "    lines = text.splitlines()\n",
    "    cleaned = []\n",
    "\n",
    "    for line in lines:\n",
    "        # Skip if line contains clinic/address info\n",
    "        if any(x in line for x in [\"clinic\", \"centre\", \"hospital\", \"#\", \"blk\", \"building\", \"road\", \"s \"]):\n",
    "            continue\n",
    "        # Skip prices and quantities\n",
    "        if re.search(r\"\\bqty\\b|\\bprice\\b|\\$\\d+|\\d+\\.\\d{2}\", line):\n",
    "            continue\n",
    "        cleaned.append(line)\n",
    "    \n",
    "\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "df = pd.read_csv(\"50_new_sample.csv\")\n",
    "df[\"extracted_text\"] = df[\"extracted_text\"].apply(preprocess_ocr_text)\n",
    "df.to_csv(\"cleaned_50_new_sample.csv\", index=False)\n",
    "\n",
    "#do not change to lowercase as it will affect the keyword matching later for medication!\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da11cc9",
   "metadata": {},
   "source": [
    "labeling of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ae329b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auto-labeling complete!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Define your CSV file and column\n",
    "csv_file = \"cleaned_50_new_sample.csv\"\n",
    "column_name = \"extracted_text\"\n",
    "\n",
    "# Define keyword groups for automatic tagging\n",
    "keywords1 = {\n",
    "    \"DOSAGE\": [\"tablet\", \"tablets\", \"teblet\", \"tab\", \"tabs\", \"tab/s\",\"cap/s\",\"cap\", \"capsule\", \"capsules\"],\n",
    "    \"FREQUENCY\": [\"times\", \"time\", \"hour\", \"hours\",\"hourly\",\"morning\", \"evening\", \"afternoon\", \"bedtime\", \"night\"]\n",
    "}\n",
    "\n",
    "keywords2 = {\n",
    "    \"FREQUENCY\": [\"once\", \"twice\"],\n",
    "    \"INSTRUCTION\": [\"when\",\"needed\", \"after\", \"use\",\"before\", \"after\", \"with\", \"without\", \"meal\", \"meals\", \"food\", \"swallow\",\"chew\"],\n",
    "    \"NOTE\" : [\"fever\",\"pain\", \"cough\", \"cold\", \"flu\", \"runny\", \"allergy\", \"infection\", \"inflammation\", \"swelling\", \"sore throat\", \"headache\", \"nausea\",\"gastric\", \"drowsiness\", \"vomiting\", \"diarrhea\", \"constipation\", \"rash\", \"itching\", \"fatigue\", \"dizziness\"]\n",
    "}\n",
    "\n",
    "\n",
    "# split sentence into tokens using regex\n",
    "def tokenize(text):\n",
    "    return re.findall(r\"\\b\\w+(?:/\\w+)?\\b\", text.lower())\n",
    "\n",
    "# Auto-label function (basic BIO tagging)\n",
    "def auto_label(tokens):\n",
    "    labels = [\"O\"] * len(tokens)\n",
    "\n",
    "    quantity_words = {\n",
    "        \"1\", \"2\", \"3\", \"4\", \"5\", \"10\", \"one\", \"two\", \"three\", \"four\", \"five\", \"half\", \"quarter\"\n",
    "    }\n",
    "\n",
    "    # Collect known keywords\n",
    "    known_keywords = set()\n",
    "    for group in [*keywords1.values(), *keywords2.values()]:\n",
    "        known_keywords.update(word.lower() for word in group)\n",
    "\n",
    "    #MEDICATION_NAME tagging to tag the first unknown word\n",
    "    for i, token in enumerate(tokens):\n",
    "        token_lower = token.lower()\n",
    "        if (\n",
    "            labels[i] == \"O\"\n",
    "            and token_lower not in known_keywords\n",
    "            and token_lower.isalpha()\n",
    "            and len(token) > 3\n",
    "        ):\n",
    "            labels[i] = \"B-MEDICATION_NAME\"\n",
    "            break  # only tag the first one\n",
    "\n",
    "    #DOSAGE tagging\n",
    "    dosage_candidates = [\n",
    "        i for i in range(1, len(tokens))\n",
    "        if tokens[i].lower() in keywords1[\"DOSAGE\"] and tokens[i - 1].lower() in quantity_words\n",
    "    ]\n",
    "\n",
    "    if len(dosage_candidates) == 1:\n",
    "        i = dosage_candidates[0]\n",
    "        labels[i - 1] = \"B-DOSAGE\"\n",
    "        labels[i] = \"I-DOSAGE\"\n",
    "    elif len(dosage_candidates) >= 2:\n",
    "        i = dosage_candidates[1]  # tag only the second\n",
    "        labels[i - 1] = \"B-DOSAGE\"\n",
    "        labels[i] = \"I-DOSAGE\"\n",
    "\n",
    "    #FREQUENCY tagging\n",
    "    for i in range(1, len(tokens)):\n",
    "        if (\n",
    "            tokens[i].lower() in keywords1[\"FREQUENCY\"] and \n",
    "            tokens[i - 1].lower()\n",
    "        ):\n",
    "            labels[i - 1] = \"B-FREQUENCY\"\n",
    "            labels[i] = \"I-FREQUENCY\"\n",
    "\n",
    "    #keywords2 tagging\n",
    "    for i, token in enumerate(tokens):\n",
    "        token_lower = token.lower()\n",
    "        for label, keyword_list in keywords2.items():\n",
    "            if token_lower in keyword_list and labels[i] == \"O\":\n",
    "                labels[i] = f\"B-{label.upper()}\"\n",
    "                if (\n",
    "                    i + 1 < len(tokens) and \n",
    "                    tokens[i + 1].lower() in keyword_list\n",
    "                ):\n",
    "                    labels[i + 1] = f\"I-{label.upper()}\"\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Clean nulls and whitespace\n",
    "df = df[df[column_name].notnull()]\n",
    "df[column_name] = df[column_name].astype(str).str.strip()\n",
    "\n",
    "# Build token-label pairs\n",
    "data = []\n",
    "for text in df[column_name]:\n",
    "    tokens = tokenize(text)\n",
    "    labels = auto_label(tokens)\n",
    "    data.append({\"tokens\": tokens, \"labels\": labels})\n",
    "\n",
    "# Save to JSON\n",
    "with open(\"auto_labeled_ner_data50.json\", \"w\") as f:\n",
    "    json.dump(data, f, indent=2)\n",
    "\n",
    "print(\"Auto-labeling complete!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1841c69",
   "metadata": {},
   "source": [
    "to do augmentation on the new 50 labels to increase to 300 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb716b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\prisc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Augmented data saved to 50augmented_data.json with 300 samples.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import os\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "#  Load original data \n",
    "with open(\"auto_labeled_ner_data50.json\", \"r\") as f:\n",
    "    original_data = json.load(f)\n",
    "\n",
    "# Get synonyms \n",
    "def get_synonyms(word):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for l in syn.lemmas():\n",
    "            synonym = l.name().replace(\"_\", \" \").lower()\n",
    "            if synonym != word.lower() and synonym.isalpha():\n",
    "                synonyms.add(synonym)\n",
    "    return list(synonyms)\n",
    "\n",
    "# Augment one sample\n",
    "def augment_sample(sample, num_aug=5):\n",
    "    tokens, labels = sample['tokens'], sample['labels']\n",
    "    augmented = []\n",
    "\n",
    "    for _ in range(num_aug):\n",
    "        new_tokens = []\n",
    "        for token, label in zip(tokens, labels):\n",
    "            if label == \"O\" and random.random() < 0.3:\n",
    "                synonyms = get_synonyms(token)\n",
    "                new_token = random.choice(synonyms) if synonyms else token\n",
    "            else:\n",
    "                new_token = token\n",
    "            new_tokens.append(new_token)\n",
    "\n",
    "        augmented.append({\n",
    "            \"tokens\": new_tokens,\n",
    "            \"labels\": labels.copy()\n",
    "        })\n",
    "\n",
    "    return augmented\n",
    "\n",
    "#  Apply to all data \n",
    "augmented_data = []\n",
    "for sample in original_data:\n",
    "    augmented_data.append(sample)  # keep original\n",
    "    augmented_data.extend(augment_sample(sample, num_aug=5))  # add 5 augmented\n",
    "\n",
    "#  Save to file \n",
    "output_file = \"50augmented_data.json\"\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(augmented_data, f, indent=2)\n",
    "\n",
    "print(f\"✅ Augmented data saved to {output_file} with {len(augmented_data)} samples.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5ccc97",
   "metadata": {},
   "source": [
    "combine clean_clean.json has 116 samples and 50augmented_data.json has 300 samples manually - combined_for_bert_training.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6eeaac1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels_for_bert_training.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"combined_for_bert_training.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Fix typo in labels\n",
    "for example in data:\n",
    "    example[\"labels\"] = [\n",
    "        \"I-INSTRUCTION\" if label == \"I-NSTRUCTION\" else label\n",
    "        for label in example[\"labels\"]\n",
    "    ]\n",
    "\n",
    "# Save fixed file\n",
    "with open(\"labels_for_bert_training.json\", \"w\") as f:\n",
    "    json.dump(data, f, indent=2)\n",
    "\n",
    "print(\"labels_for_bert_training.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "labelstudio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
